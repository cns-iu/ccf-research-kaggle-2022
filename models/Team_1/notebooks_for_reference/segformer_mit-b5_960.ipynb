{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7940,"status":"ok","timestamp":1663663195686,"user":{"displayName":"vov nanda","userId":"09231748423501225228"},"user_tz":-540},"id":"9NCETveqlIgi","outputId":"2048b0d2-66d8-484a-b563-b49f609eda0a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Sep 20 08:39:47 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["!nvidia-smi\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1663663195687,"user":{"displayName":"vov nanda","userId":"09231748423501225228"},"user_tz":-540},"id":"UWyLrLYaNEaL","outputId":"379e9486-8688-4bc3-9751-5290ca0a71f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2020 NVIDIA Corporation\n","Built on Mon_Oct_12_20:09:46_PDT_2020\n","Cuda compilation tools, release 11.1, V11.1.105\n","Build cuda_11.1.TC455_06.29190527_0\n","gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n","Copyright (C) 2017 Free Software Foundation, Inc.\n","This is free software; see the source for copying conditions.  There is NO\n","warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n","\n"]}],"source":["# CUDA version:\n","!nvcc -V\n","# Check GCC version:\n","!gcc --version\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":181253,"status":"ok","timestamp":1663663376922,"user":{"displayName":"vov nanda","userId":"09231748423501225228"},"user_tz":-540},"id":"Ki3WUBjKbutg","outputId":"d934fd7d-60d1-4aa7-c7ba-ab30111dfe2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.10.0+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2137.6 MB)\n","\u001b[K     |████████████▌                   | 834.1 MB 110.8 MB/s eta 0:00:12tcmalloc: large alloc 1147494400 bytes == 0x38f3e000 @  0x7fb7c8ebf615 0x58e046 0x4f2e5e 0x4d19df 0x51b31c 0x5b41c5 0x58f49e 0x51b221 0x5b41c5 0x58f49e 0x51837f 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4ba70a 0x538136 0x590055 0x51b180 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x58f49e 0x51740e 0x58f2a7 0x517947 0x5b41c5 0x58f49e\n","\u001b[K     |███████████████▉                | 1055.7 MB 1.2 MB/s eta 0:15:37tcmalloc: large alloc 1434370048 bytes == 0x7d594000 @  0x7fb7c8ebf615 0x58e046 0x4f2e5e 0x4d19df 0x51b31c 0x5b41c5 0x58f49e 0x51b221 0x5b41c5 0x58f49e 0x51837f 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4ba70a 0x538136 0x590055 0x51b180 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x58f49e 0x51740e 0x58f2a7 0x517947 0x5b41c5 0x58f49e\n","\u001b[K     |████████████████████            | 1336.2 MB 118.7 MB/s eta 0:00:07tcmalloc: large alloc 1792966656 bytes == 0x23c6000 @  0x7fb7c8ebf615 0x58e046 0x4f2e5e 0x4d19df 0x51b31c 0x5b41c5 0x58f49e 0x51b221 0x5b41c5 0x58f49e 0x51837f 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4ba70a 0x538136 0x590055 0x51b180 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x58f49e 0x51740e 0x58f2a7 0x517947 0x5b41c5 0x58f49e\n","\u001b[K     |█████████████████████████▎      | 1691.1 MB 1.1 MB/s eta 0:06:31tcmalloc: large alloc 2241208320 bytes == 0x6d1ae000 @  0x7fb7c8ebf615 0x58e046 0x4f2e5e 0x4d19df 0x51b31c 0x5b41c5 0x58f49e 0x51b221 0x5b41c5 0x58f49e 0x51837f 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4ba70a 0x538136 0x590055 0x51b180 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x58f49e 0x51740e 0x58f2a7 0x517947 0x5b41c5 0x58f49e\n","\u001b[K     |████████████████████████████████| 2137.6 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 2137645056 bytes == 0xf2b10000 @  0x7fb7c8ebe1e7 0x4b2150 0x4b21dc 0x58e046 0x4f2e5e 0x4d19df 0x51b31c 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x58f2a7 0x517947 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x58f49e 0x51837f 0x5b41c5\n","tcmalloc: large alloc 2672058368 bytes == 0x1e6664000 @  0x7fb7c8ebf615 0x58e046 0x4f2e5e 0x4d19df 0x51b31c 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x5b41c5 0x58f49e 0x517947 0x58f2a7 0x517947 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x4ba899 0x4d29f9\n","\u001b[K     |████████████████████████████████| 2137.6 MB 382 bytes/s \n","\u001b[?25hCollecting torchvision==0.11.0+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.11.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (21.9 MB)\n","\u001b[K     |████████████████████████████████| 21.9 MB 25.3 MB/s \n","\u001b[?25hCollecting torchaudio==0.10.0\n","  Downloading https://download.pytorch.org/whl/rocm4.1/torchaudio-0.10.0%2Brocm4.1-cp37-cp37m-linux_x86_64.whl (2.7 MB)\n","\u001b[K     |████████████████████████████████| 2.7 MB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0+cu111) (4.1.1)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.0+cu111) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.0+cu111) (1.21.6)\n","Installing collected packages: torch, torchvision, torchaudio\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.13.1+cu113\n","    Uninstalling torchvision-0.13.1+cu113:\n","      Successfully uninstalled torchvision-0.13.1+cu113\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 0.12.1+cu113\n","    Uninstalling torchaudio-0.12.1+cu113:\n","      Successfully uninstalled torchaudio-0.12.1+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.10.0+cu111 which is incompatible.\u001b[0m\n","Successfully installed torch-1.10.0+cu111 torchaudio-0.10.0+rocm4.1 torchvision-0.11.0+cu111\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.openmmlab.com/mmcv/dist/cu111/torch1.10/index.html\n","Collecting mmcv-full==1.6.0\n","  Downloading https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/mmcv_full-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (47.1 MB)\n","\u001b[K     |████████████████████████████████| 47.1 MB 12.5 MB/s \n","\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.6.0) (7.1.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.6.0) (6.0)\n","Collecting yapf\n","  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n","\u001b[K     |████████████████████████████████| 190 kB 14.4 MB/s \n","\u001b[?25hRequirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.6.0) (4.6.0.66)\n","Collecting addict\n","  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.6.0) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.6.0) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mmcv-full==1.6.0) (3.0.9)\n","Installing collected packages: yapf, addict, mmcv-full\n","Successfully installed addict-2.4.0 mmcv-full-1.6.0 yapf-0.32.0\n"]}],"source":["!pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 torchaudio==0.10.0 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","cu_version='cu111'\n","torch_version='torch1.10'\n","\n","!pip install mmcv-full==1.6.0 -f https://download.openmmlab.com/mmcv/dist/{cu_version}/{torch_version}/index.html\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9535,"status":"ok","timestamp":1663663386448,"user":{"displayName":"vov nanda","userId":"09231748423501225228"},"user_tz":-540},"id":"nR-hHRvbNJJZ","outputId":"73fa21e4-a751-420b-bc82-ab5703894226"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n","Cloning into 'mmsegmentation'...\n","remote: Enumerating objects: 7677, done.\u001b[K\n","remote: Counting objects: 100% (6/6), done.\u001b[K\n","remote: Compressing objects: 100% (6/6), done.\u001b[K\n","remote: Total 7677 (delta 0), reused 4 (delta 0), pack-reused 7671\u001b[K\n","Receiving objects: 100% (7677/7677), 13.51 MiB | 19.53 MiB/s, done.\n","Resolving deltas: 100% (5684/5684), done.\n","/content/mmsegmentation\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/mmsegmentation\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mmsegmentation==0.27.0) (3.2.2)\n","Collecting mmcls>=0.20.1\n","  Downloading mmcls-0.23.2-py2.py3-none-any.whl (578 kB)\n","\u001b[K     |████████████████████████████████| 578 kB 14.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmsegmentation==0.27.0) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mmsegmentation==0.27.0) (21.3)\n","Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from mmsegmentation==0.27.0) (3.4.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmsegmentation==0.27.0) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmsegmentation==0.27.0) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmsegmentation==0.27.0) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmsegmentation==0.27.0) (1.4.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mmsegmentation==0.27.0) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mmsegmentation==0.27.0) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable->mmsegmentation==0.27.0) (0.2.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from prettytable->mmsegmentation==0.27.0) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable->mmsegmentation==0.27.0) (3.8.1)\n","Installing collected packages: mmcls, mmsegmentation\n","  Running setup.py develop for mmsegmentation\n","Successfully installed mmcls-0.23.2 mmsegmentation-0.27.0\n","sys.platform: linux\n","Python: 3.7.14 (default, Sep  8 2022, 00:06:44) [GCC 7.5.0]\n","CUDA available: True\n","GPU 0: Tesla P100-PCIE-16GB\n","CUDA_HOME: /usr/local/cuda\n","NVCC: Cuda compilation tools, release 11.1, V11.1.105\n","GCC: x86_64-linux-gnu-gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n","PyTorch: 1.10.0+cu111\n","PyTorch compiling details: PyTorch built with:\n","  - GCC 7.3\n","  - C++ Version: 201402\n","  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n","  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)\n","  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n","  - LAPACK is enabled (usually provided by MKL)\n","  - NNPACK is enabled\n","  - CPU capability usage: AVX512\n","  - CUDA Runtime 11.1\n","  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n","  - CuDNN 8.0.5\n","  - Magma 2.5.2\n","  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n","\n","TorchVision: 0.11.0+cu111\n","OpenCV: 4.6.0\n","MMCV: 1.6.0\n","MMCV Compiler: GCC 7.3\n","MMCV CUDA Compiler: 11.1\n","MMSegmentation: 0.27.0+c1f2252\n"]}],"source":["%cd /content\n","!rm -rf mmsegmentation\n","!git clone https://github.com/ykawa2/mmsegmentation.git\n","%cd mmsegmentation\n","!pip install -e .\n","\n","!python mmseg/utils/collect_env.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2063,"status":"ok","timestamp":1663663388502,"user":{"displayName":"vov nanda","userId":"09231748423501225228"},"user_tz":-540},"id":"mAE_h7XhPT7d","outputId":"23e2fc6e-9785-4b1b-be3d-75c52fd128b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch version: 1.10.0+cu111 cuda_availabe: True\n","torchvision version: 0.11.0+cu111\n","mmseg version: 0.27.0\n","mmcv version: 1.6.0\n"]}],"source":["# Check Pytorch installation\n","import torch, torchvision\n","print('torch version:', torch.__version__, 'cuda_availabe:', torch.cuda.is_available())\n","print('torchvision version:', torchvision.__version__)\n","\n","import mmseg\n","print('mmseg version:', mmseg.__version__)\n","\n","import mmcv\n","print('mmcv version:', mmcv.__version__)"]},{"cell_type":"markdown","metadata":{"id":"y40sFkCPf_ff"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":108044,"status":"ok","timestamp":1663663496531,"user":{"displayName":"vov nanda","userId":"09231748423501225228"},"user_tz":-540},"id":"H91XUYoitDKS","outputId":"6833fee3-fa28-40b0-f3f7-0db63d33c3c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["%cd /content\n","!tar -xvf /content/drive/MyDrive/kaggle/multi_class_dataset/hubmap_2000x2000_aug_v5.tar > /dev/null\n","!mv /content/hubmap_2000x2000 /content/hubmap_multi_2000x2000"]},{"cell_type":"markdown","metadata":{"id":"kRqIHManj-Sh"},"source":["# Merge of mask patch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XRyGg5YokBLq"},"outputs":[],"source":["!cp -r /content/drive/MyDrive/kaggle/multi_class_dataset/converted_mask_patch_v3 /content"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":907,"status":"ok","timestamp":1663663510577,"user":{"displayName":"vov nanda","userId":"09231748423501225228"},"user_tz":-540},"id":"edpkFAjqkJl8","outputId":"e92ff38e-44a3-42cd-b742-5e43e8b1e9c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/hubmap_multi_2000x2000/lung_refined_masks/203.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/24194.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/12471.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/5287.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/4802.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/28318.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/28622.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/19569.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/14183.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/676.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/17143.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/3409.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/15732.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/9231.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/29180.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/12174.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/29307.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/20247.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/8222.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/8388.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/32412.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/8402.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/16659.png\n","/content/hubmap_multi_2000x2000/lung_refined_masks/5317.png\n","24\n"]}],"source":["# converted_mask_patchにあるpngファイル名と同一名のpngファイルとマスクのマージを行う\n","import glob\n","import os\n","from PIL import Image\n","import numpy as np\n","\n","target_index=[203,676,3409,4802,5287,8222,8402,9231,12174,12471,14183,15732,16659,\n","            20247,24194,28318,28622,29180,32412,29307, 8388,17143, 19569, 5317]\n","replace_index=[]\n","#replace_index=[29307, 8388]\n","#no_target=[17143, 19569, 5317]\n","\n","patch_dir='/content/converted_mask_patch_v3'\n","mask_dir='/content/hubmap_multi_2000x2000/lung_refined_masks'\n","\n","\n","files=glob.glob(patch_dir + '/*.png')\n","basename_without_ext=[os.path.basename(f).split('.')[0] for f in files ]\n","\n","cnt=0\n","for idx in basename_without_ext:\n","    if int(idx) in target_index:\n","        patch_path=os.path.join(patch_dir, idx + '.png')\n","        mask_path=os.path.join(mask_dir, idx + '.png')\n","\n","        mask=Image.open(mask_path)\n","        assert mask.mode=='P'\n","        palette=mask.getpalette()\n","        mask=np.array(mask)\n","\n","        patch=Image.open(patch_path)\n","        assert patch.mode=='P'\n","        patch=np.array(patch)\n","\n","        assert mask.shape==patch.shape\n","        \n","        mask[patch>0]=0\n","        mask+=patch\n","\n","        mask=Image.fromarray(mask)\n","        mask.putpalette(palette)\n","        mask.save(mask_path)\n","\n","    elif int(idx) in replace_index:\n","        patch_path=os.path.join(patch_dir, idx + '.png')\n","        mask_path=os.path.join(mask_dir, idx + '.png')\n","\n","        patch=Image.open(patch_path)\n","        assert patch.mode=='P'\n","        \n","        patch.save(mask_path)\n","    \n","    else:\n","        continue\n","\n","    cnt+=1\n","    print(mask_path)\n","\n","print(cnt)\n"]},{"cell_type":"markdown","metadata":{"id":"rMy0iYLFmxAW"},"source":["# 2classデータセットの生成"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPbPZR7-mxWB"},"outputs":[],"source":["!cp -r /content/hubmap_multi_2000x2000/lung_refined_masks /content/hubmap_multi_2000x2000/lung_refined_masks_2class"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32738,"status":"ok","timestamp":1663663544434,"user":{"displayName":"vov nanda","userId":"09231748423501225228"},"user_tz":-540},"id":"_QRJawaMm3c3","outputId":"9a1cc8b0-af4d-455f-de93-8f6a14d2eddc"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/21155.png\n","1 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/4639.png\n","2 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/62.png\n","3 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/19179.png\n","4 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/3054.png\n","5 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/24241.png\n","6 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30414.png\n","7 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/5099.png\n","8 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/22310.png\n","9 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/15124.png\n","10 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/1731.png\n","11 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/5777.png\n","12 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/25472.png\n","13 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/6120.png\n","14 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/4066.png\n","15 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/20563.png\n","16 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/9358.png\n","17 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/10488.png\n","18 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/21039.png\n","19 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/21129.png\n","20 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/14674.png\n","21 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/13034.png\n","22 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/15329.png\n","23 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/11662.png\n","24 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/28052.png\n","25 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/31290.png\n","26 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/203.png\n","27 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/10912.png\n","28 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/18792.png\n","29 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/1168.png\n","30 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/9437.png\n","31 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/10611.png\n","32 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/23959.png\n","33 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/27471.png\n","34 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/10703.png\n","35 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/4777.png\n","36 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/8638.png\n","37 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/28823.png\n","38 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/9791.png\n","39 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/1850.png\n","40 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/24194.png\n","41 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/32527.png\n","42 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/4062.png\n","43 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/27298.png\n","44 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/23665.png\n","45 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/1123.png\n","46 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/26886.png\n","47 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/31675.png\n","48 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/13507.png\n","49 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/32151.png\n","50 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/22995.png\n","51 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/20302.png\n","52 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/16362.png\n","53 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/23640.png\n","54 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/22133.png\n","55 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/28791.png\n","56 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/6807.png\n","57 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/21321.png\n","58 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/29213.png\n","59 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/27879.png\n","60 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/13260.png\n","61 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/10044.png\n","62 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/3057.png\n","63 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/16609.png\n","64 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/22035.png\n","65 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/9453.png\n","66 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/24222.png\n","67 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/13189.png\n","68 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/11629.png\n","69 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/2174.png\n","70 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/12471.png\n","71 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/5287.png\n","72 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/15842.png\n","73 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/27468.png\n","74 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/16890.png\n","75 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/4802.png\n","76 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/10651.png\n","77 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/18422.png\n","78 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/1955.png\n","79 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/10274.png\n","80 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/11064.png\n","81 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/6794.png\n","82 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/11890.png\n","83 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/22718.png\n","84 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/8502.png\n","85 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/11497.png\n","86 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/16163.png\n","87 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30765.png\n","88 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/8450.png\n","89 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/6722.png\n","90 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/928.png\n","91 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/27781.png\n","92 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/12476.png\n","93 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30224.png\n","94 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/1157.png\n","95 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/21812.png\n","96 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30581.png\n","97 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/4301.png\n","98 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/32009.png\n","99 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/28429.png\n","100 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/23009.png\n","101 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/18449.png\n","102 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/16728.png\n","103 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/24522.png\n","104 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/351.png\n","105 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/28318.png\n","106 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/21358.png\n","107 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/11645.png\n","108 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/16564.png\n","109 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/1220.png\n","110 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/435.png\n","111 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/7970.png\n","112 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/8231.png\n","113 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/19507.png\n","114 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/23828.png\n","115 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/8876.png\n","116 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/16216.png\n","117 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/1229.png\n","118 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/12026.png\n","119 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/7706.png\n","120 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/9769.png\n","121 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/29424.png\n","122 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/15787.png\n","123 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/29143.png\n","124 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/27232.png\n","125 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/26982.png\n","126 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/22236.png\n","127 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/4412.png\n","128 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/27340.png\n","129 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/28436.png\n","130 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/19533.png\n","131 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30355.png\n","132 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/5102.png\n","133 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/27861.png\n","134 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/9450.png\n","135 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/8343.png\n","136 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/28622.png\n","137 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/22544.png\n","138 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/19569.png\n","139 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/27350.png\n","140 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30876.png\n","141 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30424.png\n","142 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/14407.png\n","143 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/10892.png\n","144 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/8894.png\n","145 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/10992.png\n","146 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/28657.png\n","147 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/1690.png\n","148 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/2668.png\n","149 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/4658.png\n","150 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/14183.png\n","151 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/17828.png\n","152 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/14396.png\n","153 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/1878.png\n","154 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/144.png\n","155 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/15499.png\n","156 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/164.png\n","157 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/25945.png\n","158 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/13396.png\n","159 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/24097.png\n","160 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/31800.png\n","161 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/17126.png\n","162 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/20794.png\n","163 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/737.png\n","164 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/676.png\n","165 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/29296.png\n","166 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/17187.png\n","167 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/10971.png\n","168 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/8116.png\n","169 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/32325.png\n","170 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/18426.png\n","171 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/19048.png\n","172 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/2447.png\n","173 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/127.png\n","174 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/22059.png\n","175 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/15706.png\n","176 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/660.png\n","177 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/24269.png\n","178 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/18445.png\n","179 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/26174.png\n","180 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/1500.png\n","181 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/23880.png\n","182 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/2079.png\n","183 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/28262.png\n","184 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/19360.png\n","185 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/686.png\n","186 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/6021.png\n","187 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/8151.png\n","188 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/16214.png\n","189 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/23051.png\n","190 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/7359.png\n","191 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/2500.png\n","192 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/27587.png\n","193 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/21112.png\n","194 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/15067.png\n","195 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/20831.png\n","196 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/17143.png\n","197 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/3409.png\n","198 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/15732.png\n","199 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/29223.png\n","200 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/31727.png\n","201 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/17455.png\n","202 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/16149.png\n","203 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/26664.png\n","204 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/28748.png\n","205 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/29820.png\n","206 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/5995.png\n","207 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/21021.png\n","208 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/2874.png\n","209 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/31571.png\n","210 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/20428.png\n","211 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/8842.png\n","212 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30394.png\n","213 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/10610.png\n","214 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/11448.png\n","215 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/32741.png\n","216 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/24833.png\n","217 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/31698.png\n","218 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/19084.png\n","219 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/5785.png\n","220 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30080.png\n","221 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/2943.png\n","222 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/1184.png\n","223 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/31898.png\n","224 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/26480.png\n","225 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/26319.png\n","226 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/2344.png\n","227 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/14756.png\n","228 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/9231.png\n","229 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/31733.png\n","230 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/28940.png\n","231 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30500.png\n","232 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/27803.png\n","233 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/15860.png\n","234 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/24961.png\n","235 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30474.png\n","236 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/25620.png\n","237 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/9777.png\n","238 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/13483.png\n","239 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/21086.png\n","240 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/29180.png\n","241 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30201.png\n","242 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/28963.png\n","243 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/9445.png\n","244 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/5832.png\n","245 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/22016.png\n","246 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30250.png\n","247 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/12174.png\n","248 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/28126.png\n","249 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/4265.png\n","250 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/5552.png\n","251 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/9904.png\n","252 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/31709.png\n","253 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/25430.png\n","254 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/20520.png\n","255 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/29307.png\n","256 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30294.png\n","257 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/4404.png\n","258 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/10666.png\n","259 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/6121.png\n","260 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/31139.png\n","261 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/18777.png\n","262 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/27128.png\n","263 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/29809.png\n","264 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/17422.png\n","265 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/7902.png\n","266 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/25689.png\n","267 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/2696.png\n","268 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/15551.png\n","269 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/24100.png\n","270 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/12233.png\n","271 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/16711.png\n","272 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/20247.png\n","273 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/26780.png\n","274 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/24782.png\n","275 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/18121.png\n","276 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/2424.png\n","277 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/12483.png\n","278 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/25516.png\n","279 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/3083.png\n","280 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/25298.png\n","281 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/28045.png\n","282 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/3959.png\n","283 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/23252.png\n","284 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/9470.png\n","285 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/6318.png\n","286 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/20955.png\n","287 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/22741.png\n","288 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/12784.png\n","289 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/20478.png\n","290 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/5086.png\n","291 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/8222.png\n","292 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/23760.png\n","293 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/8388.png\n","294 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/6611.png\n","295 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/9387.png\n","296 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/32412.png\n","297 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/18900.png\n","298 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/8752.png\n","299 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/29610.png\n","300 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30194.png\n","301 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/7169.png\n","302 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/12244.png\n","303 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/12827.png\n","304 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/19377.png\n","305 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/25641.png\n","306 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/13942.png\n","307 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/23243.png\n","308 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/6730.png\n","309 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/31958.png\n","310 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/21501.png\n","311 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/8227.png\n","312 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/30084.png\n","313 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/10392.png\n","314 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/19997.png\n","315 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/4561.png\n","316 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/5932.png\n","317 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/31799.png\n","318 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/27616.png\n","319 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/9517.png\n","320 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/8402.png\n","321 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/2793.png\n","322 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/7397.png\n","323 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/7569.png\n","324 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/32231.png\n","325 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/14388.png\n","326 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/15192.png\n","327 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/12452.png\n","328 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/26101.png\n","329 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/3303.png\n","330 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/12466.png\n","331 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/32126.png\n","332 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/29238.png\n","333 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/23961.png\n","334 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/29690.png\n","335 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/23094.png\n","336 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/16659.png\n","337 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/18401.png\n","338 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/31406.png\n","339 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/20440.png\n","340 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/5583.png\n","341 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/28189.png\n","342 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/22953.png\n","343 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/4776.png\n","344 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/2279.png\n","345 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/6390.png\n","346 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/21195.png\n","347 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/4944.png\n","348 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/5317.png\n","349 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/9407.png\n","350 /content/hubmap_multi_2000x2000/lung_refined_masks_2class/15005.png\n"]}],"source":["import glob\n","from PIL import Image\n","import numpy as np\n","\n","from mmseg.mylib.seg_utils import convert_rgb_to_voc_palette\n","\n","files=glob.glob('/content/hubmap_multi_2000x2000/lung_refined_masks_2class/*.png')\n","\n","for idx, f in enumerate(files):\n","    pil_mask=Image.open(f)\n","    assert pil_mask.mode=='P'\n","\n","    palette=pil_mask.getpalette()\n","\n","    mask=np.asarray(pil_mask)\n","    mask=np.where(mask>=1, 1, 0)\n","    mask=mask.astype(np.uint8)\n","\n","    new_mask=Image.fromarray(mask)\n","    new_mask.putpalette(palette)\n","    assert new_mask.mode=='P', new_mask.mode\n","    unique=list(np.unique(np.asarray(new_mask)))\n","    assert unique==[0,1] or unique==[0], unique\n","    new_mask.save(f)\n","\n","    print(idx, f)"]},{"cell_type":"markdown","metadata":{"id":"TRrZaFYmicJ3"},"source":["# external spleen dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12720,"status":"ok","timestamp":1663663557134,"user":{"displayName":"vov nanda","userId":"09231748423501225228"},"user_tz":-540},"id":"amZgth88ibA_","outputId":"fd3e1192-0ba7-4137-e1e5-379c54fe3eee"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n","external_spleen_v2/\n","external_spleen_v2/image/\n","external_spleen_v2/image/stain4_24.png\n","external_spleen_v2/image/stain3_4.png\n","external_spleen_v2/image/stain4_17.png\n","external_spleen_v2/image/25.png\n","external_spleen_v2/image/stain4_14.png\n","external_spleen_v2/image/stain3_0.png\n","external_spleen_v2/image/stain3_22.png\n","external_spleen_v2/image/stain2_18.png\n","external_spleen_v2/image/stain1_15.png\n","external_spleen_v2/image/stain2_15.png\n","external_spleen_v2/image/stain1_1.png\n","external_spleen_v2/image/stain1_0.png\n","external_spleen_v2/image/stain3_31.png\n","external_spleen_v2/image/stain2_24.png\n","external_spleen_v2/image/stain3_29.png\n","external_spleen_v2/image/stain3_12.png\n","external_spleen_v2/image/stain1_18.png\n","external_spleen_v2/image/stain2_37.png\n","external_spleen_v2/image/stain2_2.png\n","external_spleen_v2/image/stain4_12.png\n","external_spleen_v2/image/stain3_7.png\n","external_spleen_v2/image/6.png\n","external_spleen_v2/image/stain2_20.png\n","external_spleen_v2/image/stain4_28.png\n","external_spleen_v2/image/33.png\n","external_spleen_v2/image/stain2_25.png\n","external_spleen_v2/image/stain4_1.png\n","external_spleen_v2/image/stain2_10078.png\n","external_spleen_v2/image/stain1_35.png\n","external_spleen_v2/image/stain3_35.png\n","external_spleen_v2/image/stain3_17.png\n","external_spleen_v2/image/stain1_28.png\n","external_spleen_v2/image/stain4_26.png\n","external_spleen_v2/image/stain2_33.png\n","external_spleen_v2/image/stain1_23.png\n","external_spleen_v2/image/stain2_0.png\n","external_spleen_v2/image/stain1_16.png\n","external_spleen_v2/image/stain2_28.png\n","external_spleen_v2/image/stain4_18.png\n","external_spleen_v2/image/stain3_8.png\n","external_spleen_v2/image/stain3_14.png\n","external_spleen_v2/image/1.png\n","external_spleen_v2/image/3.png\n","external_spleen_v2/image/stain4_32.png\n","external_spleen_v2/image/stain4_15.png\n","external_spleen_v2/image/stain4_3.png\n","external_spleen_v2/image/stain4_29.png\n","external_spleen_v2/image/stain3_34.png\n","external_spleen_v2/image/26.png\n","external_spleen_v2/image/stain1_30.png\n","external_spleen_v2/image/stain1_24.png\n","external_spleen_v2/image/stain1_25.png\n","external_spleen_v2/image/stain2_30.png\n","external_spleen_v2/image/stain4_2.png\n","external_spleen_v2/image/stain3_20.png\n","external_spleen_v2/image/0.png\n","external_spleen_v2/image/stain3_27.png\n","external_spleen_v2/image/stain2_21.png\n","external_spleen_v2/image/stain2_23.png\n","external_spleen_v2/image/stain4_9.png\n","external_spleen_v2/image/stain4_27.png\n","external_spleen_v2/image/stain3_37.png\n","external_spleen_v2/image/stain4_33.png\n","external_spleen_v2/image/8.png\n","external_spleen_v2/image/stain2_5.png\n","external_spleen_v2/image/stain1_19.png\n","external_spleen_v2/image/stain3_13.png\n","external_spleen_v2/image/stain4_31.png\n","external_spleen_v2/image/stain2_17.png\n","external_spleen_v2/image/stain4_7.png\n","external_spleen_v2/image/stain1_6.png\n","external_spleen_v2/image/stain4_35.png\n","external_spleen_v2/image/stain4_36.png\n","external_spleen_v2/image/37.png\n","external_spleen_v2/image/10.png\n","external_spleen_v2/image/11.png\n","external_spleen_v2/image/stain2_31.png\n","external_spleen_v2/image/14.png\n","external_spleen_v2/image/stain4_25.png\n","external_spleen_v2/image/9.png\n","external_spleen_v2/image/34.png\n","external_spleen_v2/image/stain4_20.png\n","external_spleen_v2/image/stain3_18.png\n","external_spleen_v2/image/stain1_2.png\n","external_spleen_v2/image/29.png\n","external_spleen_v2/image/stain2_36.png\n","external_spleen_v2/image/stain2_22.png\n","external_spleen_v2/image/stain1_9.png\n","external_spleen_v2/image/stain2_13.png\n","external_spleen_v2/image/stain4_19.png\n","external_spleen_v2/image/stain4_0.png\n","external_spleen_v2/image/stain2_32.png\n","external_spleen_v2/image/stain4_22.png\n","external_spleen_v2/image/stain1_20.png\n","external_spleen_v2/image/stain4_16.png\n","external_spleen_v2/image/stain4_37.png\n","external_spleen_v2/image/15.png\n","external_spleen_v2/image/stain4_5.png\n","external_spleen_v2/image/stain4_11.png\n","external_spleen_v2/image/stain1_5.png\n","external_spleen_v2/image/stain3_32.png\n","external_spleen_v2/image/stain4_4.png\n","external_spleen_v2/image/stain3_15.png\n","external_spleen_v2/image/stain3_10078.png\n","external_spleen_v2/image/stain4_23.png\n","external_spleen_v2/image/stain2_29.png\n","external_spleen_v2/image/stain1_29.png\n","external_spleen_v2/image/stain4_10.png\n","external_spleen_v2/image/stain3_24.png\n","external_spleen_v2/image/stain4_6.png\n","external_spleen_v2/image/stain2_19.png\n","external_spleen_v2/image/4.png\n","external_spleen_v2/image/stain1_11.png\n","external_spleen_v2/image/stain2_14.png\n","external_spleen_v2/image/stain2_35.png\n","external_spleen_v2/image/stain1_17.png\n","external_spleen_v2/image/32.png\n","external_spleen_v2/image/stain2_10.png\n","external_spleen_v2/image/stain1_14.png\n","external_spleen_v2/image/22.png\n","external_spleen_v2/image/19.png\n","external_spleen_v2/image/30.png\n","external_spleen_v2/image/stain3_23.png\n","external_spleen_v2/image/stain4_34.png\n","external_spleen_v2/image/10078.png\n","external_spleen_v2/image/stain1_3.png\n","external_spleen_v2/image/stain2_34.png\n","external_spleen_v2/image/16.png\n","external_spleen_v2/image/31.png\n","external_spleen_v2/image/stain2_9.png\n","external_spleen_v2/image/stain3_36.png\n","external_spleen_v2/image/21.png\n","external_spleen_v2/image/stain3_6.png\n","external_spleen_v2/image/stain3_10.png\n","external_spleen_v2/image/stain3_19.png\n","external_spleen_v2/image/stain3_2.png\n","external_spleen_v2/image/stain1_10.png\n","external_spleen_v2/image/stain1_26.png\n","external_spleen_v2/image/35.png\n","external_spleen_v2/image/12.png\n","external_spleen_v2/image/stain2_27.png\n","external_spleen_v2/image/stain2_8.png\n","external_spleen_v2/image/stain2_3.png\n","external_spleen_v2/image/stain1_27.png\n","external_spleen_v2/image/stain3_9.png\n","external_spleen_v2/image/stain3_28.png\n","external_spleen_v2/image/stain1_7.png\n","external_spleen_v2/image/stain1_33.png\n","external_spleen_v2/image/24.png\n","external_spleen_v2/image/stain4_8.png\n","external_spleen_v2/image/stain1_10078.png\n","external_spleen_v2/image/stain3_33.png\n","external_spleen_v2/image/stain3_3.png\n","external_spleen_v2/image/27.png\n","external_spleen_v2/image/17.png\n","external_spleen_v2/image/stain4_30.png\n","external_spleen_v2/image/stain4_21.png\n","external_spleen_v2/image/stain4_10078.png\n","external_spleen_v2/image/stain2_16.png\n","external_spleen_v2/image/stain1_36.png\n","external_spleen_v2/image/36.png\n","external_spleen_v2/image/stain1_32.png\n","external_spleen_v2/image/20.png\n","external_spleen_v2/image/stain3_11.png\n","external_spleen_v2/image/28.png\n","external_spleen_v2/image/stain3_30.png\n","external_spleen_v2/image/stain1_13.png\n","external_spleen_v2/image/2.png\n","external_spleen_v2/image/stain1_12.png\n","external_spleen_v2/image/stain3_16.png\n","external_spleen_v2/image/stain3_1.png\n","external_spleen_v2/image/23.png\n","external_spleen_v2/image/stain2_12.png\n","external_spleen_v2/image/stain2_11.png\n","external_spleen_v2/image/stain1_31.png\n","external_spleen_v2/image/stain3_5.png\n","external_spleen_v2/image/stain3_25.png\n","external_spleen_v2/image/stain1_34.png\n","external_spleen_v2/image/stain1_8.png\n","external_spleen_v2/image/stain3_26.png\n","external_spleen_v2/image/7.png\n","external_spleen_v2/image/stain2_6.png\n","external_spleen_v2/image/5.png\n","external_spleen_v2/image/13.png\n","external_spleen_v2/image/stain2_1.png\n","external_spleen_v2/image/stain4_13.png\n","external_spleen_v2/image/stain1_22.png\n","external_spleen_v2/image/stain2_7.png\n","external_spleen_v2/image/stain2_26.png\n","external_spleen_v2/image/stain2_4.png\n","external_spleen_v2/image/stain3_21.png\n","external_spleen_v2/image/stain1_37.png\n","external_spleen_v2/image/stain1_4.png\n","external_spleen_v2/image/18.png\n","external_spleen_v2/image/stain1_21.png\n","external_spleen_v2/mask/\n","external_spleen_v2/mask/stain4_24.png\n","external_spleen_v2/mask/stain3_4.png\n","external_spleen_v2/mask/stain4_17.png\n","external_spleen_v2/mask/25.png\n","external_spleen_v2/mask/stain4_14.png\n","external_spleen_v2/mask/stain3_0.png\n","external_spleen_v2/mask/stain3_22.png\n","external_spleen_v2/mask/stain2_18.png\n","external_spleen_v2/mask/stain1_15.png\n","external_spleen_v2/mask/stain2_15.png\n","external_spleen_v2/mask/stain1_1.png\n","external_spleen_v2/mask/stain1_0.png\n","external_spleen_v2/mask/stain3_31.png\n","external_spleen_v2/mask/stain2_24.png\n","external_spleen_v2/mask/stain3_29.png\n","external_spleen_v2/mask/stain3_12.png\n","external_spleen_v2/mask/stain1_18.png\n","external_spleen_v2/mask/stain2_37.png\n","external_spleen_v2/mask/stain2_2.png\n","external_spleen_v2/mask/stain4_12.png\n","external_spleen_v2/mask/stain3_7.png\n","external_spleen_v2/mask/6.png\n","external_spleen_v2/mask/stain2_20.png\n","external_spleen_v2/mask/stain4_28.png\n","external_spleen_v2/mask/33.png\n","external_spleen_v2/mask/stain2_25.png\n","external_spleen_v2/mask/stain4_1.png\n","external_spleen_v2/mask/stain2_10078.png\n","external_spleen_v2/mask/stain1_35.png\n","external_spleen_v2/mask/stain3_35.png\n","external_spleen_v2/mask/stain3_17.png\n","external_spleen_v2/mask/stain1_28.png\n","external_spleen_v2/mask/stain4_26.png\n","external_spleen_v2/mask/stain2_33.png\n","external_spleen_v2/mask/stain1_23.png\n","external_spleen_v2/mask/stain2_0.png\n","external_spleen_v2/mask/stain1_16.png\n","external_spleen_v2/mask/stain2_28.png\n","external_spleen_v2/mask/stain4_18.png\n","external_spleen_v2/mask/stain3_8.png\n","external_spleen_v2/mask/stain3_14.png\n","external_spleen_v2/mask/1.png\n","external_spleen_v2/mask/3.png\n","external_spleen_v2/mask/stain4_32.png\n","external_spleen_v2/mask/stain4_15.png\n","external_spleen_v2/mask/stain4_3.png\n","external_spleen_v2/mask/stain4_29.png\n","external_spleen_v2/mask/stain3_34.png\n","external_spleen_v2/mask/26.png\n","external_spleen_v2/mask/stain1_30.png\n","external_spleen_v2/mask/stain1_24.png\n","external_spleen_v2/mask/stain1_25.png\n","external_spleen_v2/mask/stain2_30.png\n","external_spleen_v2/mask/stain4_2.png\n","external_spleen_v2/mask/stain3_20.png\n","external_spleen_v2/mask/0.png\n","external_spleen_v2/mask/stain3_27.png\n","external_spleen_v2/mask/stain2_21.png\n","external_spleen_v2/mask/stain2_23.png\n","external_spleen_v2/mask/stain4_9.png\n","external_spleen_v2/mask/stain4_27.png\n","external_spleen_v2/mask/stain3_37.png\n","external_spleen_v2/mask/stain4_33.png\n","external_spleen_v2/mask/8.png\n","external_spleen_v2/mask/stain2_5.png\n","external_spleen_v2/mask/stain1_19.png\n","external_spleen_v2/mask/stain3_13.png\n","external_spleen_v2/mask/stain4_31.png\n","external_spleen_v2/mask/stain2_17.png\n","external_spleen_v2/mask/stain4_7.png\n","external_spleen_v2/mask/stain1_6.png\n","external_spleen_v2/mask/stain4_35.png\n","external_spleen_v2/mask/stain4_36.png\n","external_spleen_v2/mask/37.png\n","external_spleen_v2/mask/10.png\n","external_spleen_v2/mask/11.png\n","external_spleen_v2/mask/stain2_31.png\n","external_spleen_v2/mask/14.png\n","external_spleen_v2/mask/stain4_25.png\n","external_spleen_v2/mask/9.png\n","external_spleen_v2/mask/34.png\n","external_spleen_v2/mask/stain4_20.png\n","external_spleen_v2/mask/stain3_18.png\n","external_spleen_v2/mask/stain1_2.png\n","external_spleen_v2/mask/29.png\n","external_spleen_v2/mask/stain2_36.png\n","external_spleen_v2/mask/stain2_22.png\n","external_spleen_v2/mask/stain1_9.png\n","external_spleen_v2/mask/stain2_13.png\n","external_spleen_v2/mask/stain4_19.png\n","external_spleen_v2/mask/stain4_0.png\n","external_spleen_v2/mask/stain2_32.png\n","external_spleen_v2/mask/stain4_22.png\n","external_spleen_v2/mask/stain1_20.png\n","external_spleen_v2/mask/stain4_16.png\n","external_spleen_v2/mask/stain4_37.png\n","external_spleen_v2/mask/15.png\n","external_spleen_v2/mask/stain4_5.png\n","external_spleen_v2/mask/stain4_11.png\n","external_spleen_v2/mask/stain1_5.png\n","external_spleen_v2/mask/stain3_32.png\n","external_spleen_v2/mask/stain4_4.png\n","external_spleen_v2/mask/stain3_15.png\n","external_spleen_v2/mask/stain3_10078.png\n","external_spleen_v2/mask/stain4_23.png\n","external_spleen_v2/mask/stain2_29.png\n","external_spleen_v2/mask/stain1_29.png\n","external_spleen_v2/mask/stain4_10.png\n","external_spleen_v2/mask/stain3_24.png\n","external_spleen_v2/mask/stain4_6.png\n","external_spleen_v2/mask/stain2_19.png\n","external_spleen_v2/mask/4.png\n","external_spleen_v2/mask/stain1_11.png\n","external_spleen_v2/mask/stain2_14.png\n","external_spleen_v2/mask/stain2_35.png\n","external_spleen_v2/mask/stain1_17.png\n","external_spleen_v2/mask/32.png\n","external_spleen_v2/mask/stain2_10.png\n","external_spleen_v2/mask/stain1_14.png\n","external_spleen_v2/mask/22.png\n","external_spleen_v2/mask/19.png\n","external_spleen_v2/mask/30.png\n","external_spleen_v2/mask/stain3_23.png\n","external_spleen_v2/mask/stain4_34.png\n","external_spleen_v2/mask/10078.png\n","external_spleen_v2/mask/stain1_3.png\n","external_spleen_v2/mask/stain2_34.png\n","external_spleen_v2/mask/16.png\n","external_spleen_v2/mask/31.png\n","external_spleen_v2/mask/stain2_9.png\n","external_spleen_v2/mask/stain3_36.png\n","external_spleen_v2/mask/21.png\n","external_spleen_v2/mask/stain3_6.png\n","external_spleen_v2/mask/stain3_10.png\n","external_spleen_v2/mask/stain3_19.png\n","external_spleen_v2/mask/stain3_2.png\n","external_spleen_v2/mask/stain1_10.png\n","external_spleen_v2/mask/stain1_26.png\n","external_spleen_v2/mask/35.png\n","external_spleen_v2/mask/12.png\n","external_spleen_v2/mask/stain2_27.png\n","external_spleen_v2/mask/stain2_8.png\n","external_spleen_v2/mask/stain2_3.png\n","external_spleen_v2/mask/stain1_27.png\n","external_spleen_v2/mask/stain3_9.png\n","external_spleen_v2/mask/stain3_28.png\n","external_spleen_v2/mask/stain1_7.png\n","external_spleen_v2/mask/stain1_33.png\n","external_spleen_v2/mask/24.png\n","external_spleen_v2/mask/stain4_8.png\n","external_spleen_v2/mask/stain1_10078.png\n","external_spleen_v2/mask/stain3_33.png\n","external_spleen_v2/mask/stain3_3.png\n","external_spleen_v2/mask/27.png\n","external_spleen_v2/mask/17.png\n","external_spleen_v2/mask/stain4_30.png\n","external_spleen_v2/mask/stain4_21.png\n","external_spleen_v2/mask/stain4_10078.png\n","external_spleen_v2/mask/stain2_16.png\n","external_spleen_v2/mask/stain1_36.png\n","external_spleen_v2/mask/36.png\n","external_spleen_v2/mask/stain1_32.png\n","external_spleen_v2/mask/20.png\n","external_spleen_v2/mask/stain3_11.png\n","external_spleen_v2/mask/28.png\n","external_spleen_v2/mask/stain3_30.png\n","external_spleen_v2/mask/stain1_13.png\n","external_spleen_v2/mask/2.png\n","external_spleen_v2/mask/stain1_12.png\n","external_spleen_v2/mask/stain3_16.png\n","external_spleen_v2/mask/stain3_1.png\n","external_spleen_v2/mask/23.png\n","external_spleen_v2/mask/stain2_12.png\n","external_spleen_v2/mask/stain2_11.png\n","external_spleen_v2/mask/stain1_31.png\n","external_spleen_v2/mask/stain3_5.png\n","external_spleen_v2/mask/stain3_25.png\n","external_spleen_v2/mask/stain1_34.png\n","external_spleen_v2/mask/stain1_8.png\n","external_spleen_v2/mask/stain3_26.png\n","external_spleen_v2/mask/7.png\n","external_spleen_v2/mask/stain2_6.png\n","external_spleen_v2/mask/5.png\n","external_spleen_v2/mask/13.png\n","external_spleen_v2/mask/stain2_1.png\n","external_spleen_v2/mask/stain4_13.png\n","external_spleen_v2/mask/stain1_22.png\n","external_spleen_v2/mask/stain2_7.png\n","external_spleen_v2/mask/stain2_26.png\n","external_spleen_v2/mask/stain2_4.png\n","external_spleen_v2/mask/stain3_21.png\n","external_spleen_v2/mask/stain1_37.png\n","external_spleen_v2/mask/stain1_4.png\n","external_spleen_v2/mask/18.png\n","external_spleen_v2/mask/stain1_21.png\n","external_spleen_v2/trainval.txt\n","external_spleen_v2/.ipynb_checkpoints/\n"]}],"source":["%cd /content\n","!tar -xvf /content/drive/MyDrive/kaggle/multi_class_dataset/external_spleen_v2.tar"]},{"cell_type":"markdown","metadata":{"id":"42-E-S7sB182"},"source":["## external lung dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8483,"status":"ok","timestamp":1663663565602,"user":{"displayName":"vov nanda","userId":"09231748423501225228"},"user_tz":-540},"id":"_134Rr2FB2G-","outputId":"a2bf2d96-aa95-4262-df06-8c05e6025e66"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n","external_lung_v1/\n","external_lung_v1/image/\n","external_lung_v1/image/lung_39.png\n","external_lung_v1/image/lung_77.png\n","external_lung_v1/image/lung_49.png\n","external_lung_v1/image/lung_81.png\n","external_lung_v1/image/lung_72.png\n","external_lung_v1/image/lung_73.png\n","external_lung_v1/image/lung_66.png\n","external_lung_v1/image/lung_32.png\n","external_lung_v1/image/lung_22.png\n","external_lung_v1/image/lung_46.png\n","external_lung_v1/image/lung_63.png\n","external_lung_v1/image/lung_60.png\n","external_lung_v1/image/lung_70.png\n","external_lung_v1/image/lung_0.png\n","external_lung_v1/image/lung_51.png\n","external_lung_v1/image/lung_59.png\n","external_lung_v1/image/lung_45.png\n","external_lung_v1/image/lung_36.png\n","external_lung_v1/image/lung_61.png\n","external_lung_v1/image/lung_31.png\n","external_lung_v1/image/lung_14.png\n","external_lung_v1/image/lung_37.png\n","external_lung_v1/image/lung_30.png\n","external_lung_v1/image/lung_20.png\n","external_lung_v1/image/lung_25.png\n","external_lung_v1/image/lung_56.png\n","external_lung_v1/image/lung_52.png\n","external_lung_v1/image/lung_4.png\n","external_lung_v1/image/lung_5.png\n","external_lung_v1/image/lung_65.png\n","external_lung_v1/image/lung_13.png\n","external_lung_v1/image/lung_67.png\n","external_lung_v1/image/lung_8.png\n","external_lung_v1/image/lung_24.png\n","external_lung_v1/image/lung_71.png\n","external_lung_v1/image/lung_18.png\n","external_lung_v1/image/lung_23.png\n","external_lung_v1/image/lung_33.png\n","external_lung_v1/image/lung_53.png\n","external_lung_v1/image/lung_74.png\n","external_lung_v1/image/lung_68.png\n","external_lung_v1/image/lung_69.png\n","external_lung_v1/image/lung_58.png\n","external_lung_v1/image/lung_2.png\n","external_lung_v1/image/lung_28.png\n","external_lung_v1/image/lung_80.png\n","external_lung_v1/image/lung_40.png\n","external_lung_v1/image/lung_35.png\n","external_lung_v1/image/lung_27.png\n","external_lung_v1/image/lung_29.png\n","external_lung_v1/image/lung_11.png\n","external_lung_v1/image/lung_1.png\n","external_lung_v1/image/lung_54.png\n","external_lung_v1/image/lung_44.png\n","external_lung_v1/image/lung_34.png\n","external_lung_v1/image/lung_26.png\n","external_lung_v1/image/lung_43.png\n","external_lung_v1/image/lung_12.png\n","external_lung_v1/image/lung_15.png\n","external_lung_v1/image/lung_7.png\n","external_lung_v1/image/lung_6.png\n","external_lung_v1/image/lung_41.png\n","external_lung_v1/image/lung_75.png\n","external_lung_v1/image/lung_38.png\n","external_lung_v1/image/lung_76.png\n","external_lung_v1/image/lung_55.png\n","external_lung_v1/image/lung_17.png\n","external_lung_v1/image/lung_64.png\n","external_lung_v1/image/lung_16.png\n","external_lung_v1/image/lung_10.png\n","external_lung_v1/image/lung_57.png\n","external_lung_v1/image/lung_21.png\n","external_lung_v1/image/lung_3.png\n","external_lung_v1/image/lung_47.png\n","external_lung_v1/image/lung_79.png\n","external_lung_v1/image/lung_62.png\n","external_lung_v1/image/lung_78.png\n","external_lung_v1/image/lung_42.png\n","external_lung_v1/image/lung_50.png\n","external_lung_v1/image/lung_9.png\n","external_lung_v1/image/lung_48.png\n","external_lung_v1/image/lung_19.png\n","external_lung_v1/external_lung.txt\n","external_lung_v1/mask_0.3/\n","external_lung_v1/mask_0.3/lung_39.png\n","external_lung_v1/mask_0.3/lung_77.png\n","external_lung_v1/mask_0.3/lung_49.png\n","external_lung_v1/mask_0.3/lung_81.png\n","external_lung_v1/mask_0.3/lung_72.png\n","external_lung_v1/mask_0.3/lung_73.png\n","external_lung_v1/mask_0.3/lung_66.png\n","external_lung_v1/mask_0.3/lung_32.png\n","external_lung_v1/mask_0.3/lung_46.png\n","external_lung_v1/mask_0.3/lung_63.png\n","external_lung_v1/mask_0.3/lung_60.png\n","external_lung_v1/mask_0.3/lung_70.png\n","external_lung_v1/mask_0.3/lung_0.png\n","external_lung_v1/mask_0.3/lung_51.png\n","external_lung_v1/mask_0.3/lung_59.png\n","external_lung_v1/mask_0.3/lung_45.png\n","external_lung_v1/mask_0.3/lung_36.png\n","external_lung_v1/mask_0.3/lung_61.png\n","external_lung_v1/mask_0.3/lung_31.png\n","external_lung_v1/mask_0.3/lung_22.png\n","external_lung_v1/mask_0.3/lung_14.png\n","external_lung_v1/mask_0.3/lung_37.png\n","external_lung_v1/mask_0.3/lung_30.png\n","external_lung_v1/mask_0.3/lung_20.png\n","external_lung_v1/mask_0.3/lung_25.png\n","external_lung_v1/mask_0.3/lung_56.png\n","external_lung_v1/mask_0.3/lung_52.png\n","external_lung_v1/mask_0.3/lung_4.png\n","external_lung_v1/mask_0.3/lung_5.png\n","external_lung_v1/mask_0.3/lung_65.png\n","external_lung_v1/mask_0.3/lung_13.png\n","external_lung_v1/mask_0.3/lung_67.png\n","external_lung_v1/mask_0.3/lung_8.png\n","external_lung_v1/mask_0.3/lung_24.png\n","external_lung_v1/mask_0.3/lung_71.png\n","external_lung_v1/mask_0.3/lung_18.png\n","external_lung_v1/mask_0.3/lung_23.png\n","external_lung_v1/mask_0.3/lung_33.png\n","external_lung_v1/mask_0.3/lung_53.png\n","external_lung_v1/mask_0.3/lung_74.png\n","external_lung_v1/mask_0.3/lung_68.png\n","external_lung_v1/mask_0.3/lung_69.png\n","external_lung_v1/mask_0.3/lung_58.png\n","external_lung_v1/mask_0.3/lung_2.png\n","external_lung_v1/mask_0.3/lung_28.png\n","external_lung_v1/mask_0.3/lung_80.png\n","external_lung_v1/mask_0.3/lung_40.png\n","external_lung_v1/mask_0.3/lung_35.png\n","external_lung_v1/mask_0.3/lung_27.png\n","external_lung_v1/mask_0.3/lung_29.png\n","external_lung_v1/mask_0.3/lung_11.png\n","external_lung_v1/mask_0.3/lung_1.png\n","external_lung_v1/mask_0.3/lung_54.png\n","external_lung_v1/mask_0.3/lung_44.png\n","external_lung_v1/mask_0.3/lung_34.png\n","external_lung_v1/mask_0.3/lung_26.png\n","external_lung_v1/mask_0.3/lung_43.png\n","external_lung_v1/mask_0.3/lung_12.png\n","external_lung_v1/mask_0.3/lung_15.png\n","external_lung_v1/mask_0.3/lung_7.png\n","external_lung_v1/mask_0.3/lung_6.png\n","external_lung_v1/mask_0.3/lung_41.png\n","external_lung_v1/mask_0.3/lung_75.png\n","external_lung_v1/mask_0.3/lung_38.png\n","external_lung_v1/mask_0.3/lung_76.png\n","external_lung_v1/mask_0.3/lung_55.png\n","external_lung_v1/mask_0.3/lung_17.png\n","external_lung_v1/mask_0.3/lung_64.png\n","external_lung_v1/mask_0.3/lung_16.png\n","external_lung_v1/mask_0.3/lung_10.png\n","external_lung_v1/mask_0.3/lung_57.png\n","external_lung_v1/mask_0.3/lung_21.png\n","external_lung_v1/mask_0.3/lung_3.png\n","external_lung_v1/mask_0.3/lung_47.png\n","external_lung_v1/mask_0.3/lung_79.png\n","external_lung_v1/mask_0.3/lung_62.png\n","external_lung_v1/mask_0.3/lung_78.png\n","external_lung_v1/mask_0.3/lung_42.png\n","external_lung_v1/mask_0.3/lung_50.png\n","external_lung_v1/mask_0.3/lung_9.png\n","external_lung_v1/mask_0.3/lung_48.png\n","external_lung_v1/mask_0.3/lung_19.png\n","external_lung_v1/.ipynb_checkpoints/\n"]}],"source":["%cd /content\n","!tar -xvf /content/drive/MyDrive/kaggle/multi_class_dataset/external_lung_v1.tar"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"elapsed":17963,"status":"ok","timestamp":1663663583548,"user":{"displayName":"vov nanda","userId":"09231748423501225228"},"user_tz":-540},"id":"7Zp4i_Izm-pf","outputId":"42bfb5fd-dc08-476d-d552-aa01901ebb16"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[K     |████████████████████████████████| 1.8 MB 11.6 MB/s \n","\u001b[K     |████████████████████████████████| 181 kB 80.2 MB/s \n","\u001b[K     |████████████████████████████████| 158 kB 41 kB/s \n","\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 96.9 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 71.6 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 109.7 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 108.4 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 103.9 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 99.6 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 98.2 MB/s \n","\u001b[K     |████████████████████████████████| 156 kB 103.8 MB/s \n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]},{"name":"stderr","output_type":"stream","text":["ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["!pip install -q wandb\n","\n","import wandb\n","wandb.login()"]},{"cell_type":"markdown","metadata":{"id":"D8ZWOUR2Yb_2"},"source":["## mmsegmentation mDice settings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i4tS_ScxY655"},"outputs":[],"source":["%%bash\n","cat << EOF > /content/mmsegmentation/mmseg/core/evaluation/metrics.py\n","# Copyright (c) OpenMMLab. All rights reserved.\n","from collections import OrderedDict\n","\n","import mmcv\n","import numpy as np\n","import torch\n","\n","\n","def f_score(precision, recall, beta=1):\n","    \"\"\"calculate the f-score value.\n","\n","    Args:\n","        precision (float | torch.Tensor): The precision value.\n","        recall (float | torch.Tensor): The recall value.\n","        beta (int): Determines the weight of recall in the combined score.\n","            Default: False.\n","\n","    Returns:\n","        [torch.tensor]: The f-score value.\n","    \"\"\"\n","    score = (1 + beta**2) * (precision * recall) / (\n","        (beta**2 * precision) + recall)\n","    return score\n","\n","\n","def intersect_and_union(pred_label,\n","                        label,\n","                        num_classes,\n","                        ignore_index,\n","                        label_map=dict(),\n","                        reduce_zero_label=False):\n","    \"\"\"Calculate intersection and Union.\n","\n","    Args:\n","        pred_label (ndarray | str): Prediction segmentation map\n","            or predict result filename.\n","        label (ndarray | str): Ground truth segmentation map\n","            or label filename.\n","        num_classes (int): Number of categories.\n","        ignore_index (int): Index that will be ignored in evaluation.\n","        label_map (dict): Mapping old labels to new labels. The parameter will\n","            work only when label is str. Default: dict().\n","        reduce_zero_label (bool): Whether ignore zero label. The parameter will\n","            work only when label is str. Default: False.\n","\n","     Returns:\n","         torch.Tensor: The intersection of prediction and ground truth\n","            histogram on all classes.\n","         torch.Tensor: The union of prediction and ground truth histogram on\n","            all classes.\n","         torch.Tensor: The prediction histogram on all classes.\n","         torch.Tensor: The ground truth histogram on all classes.\n","    \"\"\"\n","\n","    if isinstance(pred_label, str):\n","        pred_label = torch.from_numpy(np.load(pred_label))\n","    else:\n","        pred_label = torch.from_numpy((pred_label))\n","\n","    if isinstance(label, str):\n","        label = torch.from_numpy(\n","            mmcv.imread(label, flag='unchanged', backend='pillow'))\n","    else:\n","        label = torch.from_numpy(label)\n","\n","    if label_map is not None:\n","        for old_id, new_id in label_map.items():\n","            label[label == old_id] = new_id\n","    if reduce_zero_label:\n","        label[label == 0] = 255\n","        label = label - 1\n","        label[label == 254] = 255\n","\n","    mask = (label != ignore_index)\n","    pred_label = pred_label[mask]\n","    label = label[mask]\n","\n","    intersect = pred_label[pred_label == label]\n","    area_intersect = torch.histc(\n","        intersect.float(), bins=(num_classes), min=0, max=num_classes - 1)\n","    area_pred_label = torch.histc(\n","        pred_label.float(), bins=(num_classes), min=0, max=num_classes - 1)\n","    area_label = torch.histc(\n","        label.float(), bins=(num_classes), min=0, max=num_classes - 1)\n","    area_union = area_pred_label + area_label - area_intersect\n","    return area_intersect, area_union, area_pred_label, area_label\n","\n","\n","def total_intersect_and_union(results,\n","                              gt_seg_maps,\n","                              num_classes,\n","                              ignore_index,\n","                              label_map=dict(),\n","                              reduce_zero_label=False,\n","                              ):\n","    \"\"\"Calculate Total Intersection and Union.\n","\n","    Args:\n","        results (list[ndarray] | list[str]): List of prediction segmentation\n","            maps or list of prediction result filenames.\n","        gt_seg_maps (list[ndarray] | list[str] | Iterables): list of ground\n","            truth segmentation maps or list of label filenames.\n","        num_classes (int): Number of categories.\n","        ignore_index (int): Index that will be ignored in evaluation.\n","        label_map (dict): Mapping old labels to new labels. Default: dict().\n","        reduce_zero_label (bool): Whether ignore zero label. Default: False.\n","\n","     Returns:\n","         ndarray: The intersection of prediction and ground truth histogram\n","             on all classes.\n","         ndarray: The union of prediction and ground truth histogram on all\n","             classes.\n","         ndarray: The prediction histogram on all classes.\n","         ndarray: The ground truth histogram on all classes.\n","    \"\"\"\n","\n","    print('excuting <total_intersect_and_union>')\n","    total_area_intersect = torch.zeros((num_classes, ), dtype=torch.float64)\n","    total_area_union = torch.zeros((num_classes, ), dtype=torch.float64)\n","    total_area_pred_label = torch.zeros((num_classes, ), dtype=torch.float64)\n","    total_area_label = torch.zeros((num_classes, ), dtype=torch.float64)\n","    per_img_mdice = torch.zeros((num_classes, ), dtype=torch.float64)\n","\n","    cnt = 0\n","    for result, gt_seg_map in zip(results, gt_seg_maps):\n","        area_intersect, area_union, area_pred_label, area_label = \\\n","            intersect_and_union(\n","                result, gt_seg_map, num_classes, ignore_index,\n","                label_map, reduce_zero_label)\n","        total_area_intersect += area_intersect\n","        total_area_union += area_union\n","        total_area_pred_label += area_pred_label\n","        total_area_label += area_label\n","\n","        dice_coefficient = 2 * area_intersect / (area_pred_label + area_label)\n","        per_img_mdice += dice_coefficient\n","        cnt += 1\n","\n","    per_img_mdice /= cnt\n","    return total_area_intersect, total_area_union, total_area_pred_label, \\\n","        total_area_label, per_img_mdice\n","\n","\n","def mean_iou(results,\n","             gt_seg_maps,\n","             num_classes,\n","             ignore_index,\n","             nan_to_num=None,\n","             label_map=dict(),\n","             reduce_zero_label=False):\n","    \"\"\"Calculate Mean Intersection and Union (mIoU)\n","\n","    Args:\n","        results (list[ndarray] | list[str]): List of prediction segmentation\n","            maps or list of prediction result filenames.\n","        gt_seg_maps (list[ndarray] | list[str]): list of ground truth\n","            segmentation maps or list of label filenames.\n","        num_classes (int): Number of categories.\n","        ignore_index (int): Index that will be ignored in evaluation.\n","        nan_to_num (int, optional): If specified, NaN values will be replaced\n","            by the numbers defined by the user. Default: None.\n","        label_map (dict): Mapping old labels to new labels. Default: dict().\n","        reduce_zero_label (bool): Whether ignore zero label. Default: False.\n","\n","     Returns:\n","        dict[str, float | ndarray]:\n","            <aAcc> float: Overall accuracy on all images.\n","            <Acc> ndarray: Per category accuracy, shape (num_classes, ).\n","            <IoU> ndarray: Per category IoU, shape (num_classes, ).\n","    \"\"\"\n","    iou_result = eval_metrics(\n","        results=results,\n","        gt_seg_maps=gt_seg_maps,\n","        num_classes=num_classes,\n","        ignore_index=ignore_index,\n","        metrics=['mIoU'],\n","        nan_to_num=nan_to_num,\n","        label_map=label_map,\n","        reduce_zero_label=reduce_zero_label)\n","    return iou_result\n","\n","\n","def mean_dice(results,\n","              gt_seg_maps,\n","              num_classes,\n","              ignore_index,\n","              nan_to_num=None,\n","              label_map=dict(),\n","              reduce_zero_label=False):\n","    \"\"\"Calculate Mean Dice (mDice)\n","\n","    Args:\n","        results (list[ndarray] | list[str]): List of prediction segmentation\n","            maps or list of prediction result filenames.\n","        gt_seg_maps (list[ndarray] | list[str]): list of ground truth\n","            segmentation maps or list of label filenames.\n","        num_classes (int): Number of categories.\n","        ignore_index (int): Index that will be ignored in evaluation.\n","        nan_to_num (int, optional): If specified, NaN values will be replaced\n","            by the numbers defined by the user. Default: None.\n","        label_map (dict): Mapping old labels to new labels. Default: dict().\n","        reduce_zero_label (bool): Whether ignore zero label. Default: False.\n","\n","     Returns:\n","        dict[str, float | ndarray]: Default metrics.\n","            <aAcc> float: Overall accuracy on all images.\n","            <Acc> ndarray: Per category accuracy, shape (num_classes, ).\n","            <Dice> ndarray: Per category dice, shape (num_classes, ).\n","    \"\"\"\n","\n","    dice_result = eval_metrics(\n","        results=results,\n","        gt_seg_maps=gt_seg_maps,\n","        num_classes=num_classes,\n","        ignore_index=ignore_index,\n","        metrics=['mDice'],\n","        nan_to_num=nan_to_num,\n","        label_map=label_map,\n","        reduce_zero_label=reduce_zero_label)\n","    return dice_result\n","\n","\n","def mean_fscore(results,\n","                gt_seg_maps,\n","                num_classes,\n","                ignore_index,\n","                nan_to_num=None,\n","                label_map=dict(),\n","                reduce_zero_label=False,\n","                beta=1):\n","    \"\"\"Calculate Mean F-Score (mFscore)\n","\n","    Args:\n","        results (list[ndarray] | list[str]): List of prediction segmentation\n","            maps or list of prediction result filenames.\n","        gt_seg_maps (list[ndarray] | list[str]): list of ground truth\n","            segmentation maps or list of label filenames.\n","        num_classes (int): Number of categories.\n","        ignore_index (int): Index that will be ignored in evaluation.\n","        nan_to_num (int, optional): If specified, NaN values will be replaced\n","            by the numbers defined by the user. Default: None.\n","        label_map (dict): Mapping old labels to new labels. Default: dict().\n","        reduce_zero_label (bool): Whether ignore zero label. Default: False.\n","        beta (int): Determines the weight of recall in the combined score.\n","            Default: False.\n","\n","\n","     Returns:\n","        dict[str, float | ndarray]: Default metrics.\n","            <aAcc> float: Overall accuracy on all images.\n","            <Fscore> ndarray: Per category recall, shape (num_classes, ).\n","            <Precision> ndarray: Per category precision, shape (num_classes, ).\n","            <Recall> ndarray: Per category f-score, shape (num_classes, ).\n","    \"\"\"\n","    fscore_result = eval_metrics(\n","        results=results,\n","        gt_seg_maps=gt_seg_maps,\n","        num_classes=num_classes,\n","        ignore_index=ignore_index,\n","        metrics=['mFscore'],\n","        nan_to_num=nan_to_num,\n","        label_map=label_map,\n","        reduce_zero_label=reduce_zero_label,\n","        beta=beta)\n","    return fscore_result\n","\n","\n","def eval_metrics(results,\n","                 gt_seg_maps,\n","                 num_classes,\n","                 ignore_index,\n","                 metrics=['mIoU'],\n","                 nan_to_num=None,\n","                 label_map=dict(),\n","                 reduce_zero_label=False,\n","                 beta=1):\n","    \"\"\"Calculate evaluation metrics\n","    Args:\n","        results (list[ndarray] | list[str]): List of prediction segmentation\n","            maps or list of prediction result filenames.\n","        gt_seg_maps (list[ndarray] | list[str] | Iterables): list of ground\n","            truth segmentation maps or list of label filenames.\n","        num_classes (int): Number of categories.\n","        ignore_index (int): Index that will be ignored in evaluation.\n","        metrics (list[str] | str): Metrics to be evaluated, 'mIoU' and 'mDice'.\n","        nan_to_num (int, optional): If specified, NaN values will be replaced\n","            by the numbers defined by the user. Default: None.\n","        label_map (dict): Mapping old labels to new labels. Default: dict().\n","        reduce_zero_label (bool): Whether ignore zero label. Default: False.\n","     Returns:\n","        float: Overall accuracy on all images.\n","        ndarray: Per category accuracy, shape (num_classes, ).\n","        ndarray: Per category evaluation metrics, shape (num_classes, ).\n","    \"\"\"\n","\n","    total_area_intersect, total_area_union, total_area_pred_label, \\\n","        total_area_label, per_img_mdice = total_intersect_and_union(\n","            results, gt_seg_maps, num_classes, ignore_index, label_map,\n","            reduce_zero_label)\n","    ret_metrics = total_area_to_metrics(total_area_intersect, total_area_union,\n","                                        total_area_pred_label,\n","                                        total_area_label,\n","                                        per_img_mdice,\n","                                        metrics, nan_to_num,\n","                                        beta)\n","\n","    return ret_metrics\n","\n","\n","def pre_eval_to_metrics(pre_eval_results,\n","                        metrics=['mIoU'],\n","                        nan_to_num=None,\n","                        beta=1):\n","    \"\"\"Convert pre-eval results to metrics.\n","\n","    Args:\n","        pre_eval_results (list[tuple[torch.Tensor]]): per image eval results\n","            for computing evaluation metric\n","        metrics (list[str] | str): Metrics to be evaluated, 'mIoU' and 'mDice'.\n","        nan_to_num (int, optional): If specified, NaN values will be replaced\n","            by the numbers defined by the user. Default: None.\n","     Returns:\n","        float: Overall accuracy on all images.\n","        ndarray: Per category accuracy, shape (num_classes, ).\n","        ndarray: Per category evaluation metrics, shape (num_classes, ).\n","    \"\"\"\n","\n","    # convert list of tuples to tuple of lists, e.g.\n","    # [(A_1, B_1, C_1, D_1), ...,  (A_n, B_n, C_n, D_n)] to\n","    # ([A_1, ..., A_n], ..., [D_1, ..., D_n])\n","    print('\\nexcuting <pre_eval_to_metrics>')\n","    pre_eval_results = tuple(zip(*pre_eval_results))\n","    assert len(pre_eval_results) == 4\n","\n","    total_area_intersect = sum(pre_eval_results[0])\n","    total_area_union = sum(pre_eval_results[1])\n","    total_area_pred_label = sum(pre_eval_results[2])\n","    total_area_label = sum(pre_eval_results[3])\n","    per_img_mdice = None  # sum(pre_eval_results[4])\n","\n","    ret_metrics = total_area_to_metrics(total_area_intersect, total_area_union,\n","                                        total_area_pred_label,\n","                                        total_area_label,\n","                                        per_img_mdice,\n","                                        metrics, nan_to_num,\n","                                        beta)\n","\n","    return ret_metrics\n","\n","\n","def total_area_to_metrics(total_area_intersect,\n","                          total_area_union,\n","                          total_area_pred_label,\n","                          total_area_label,\n","                          per_img_mdice,\n","                          metrics=['mIoU'],\n","                          nan_to_num=None,\n","                          beta=1):\n","    \"\"\"Calculate evaluation metrics\n","    Args:\n","        total_area_intersect (ndarray): The intersection of prediction and\n","            ground truth histogram on all classes.\n","        total_area_union (ndarray): The union of prediction and ground truth\n","            histogram on all classes.\n","        total_area_pred_label (ndarray): The prediction histogram on all\n","            classes.\n","        total_area_label (ndarray): The ground truth histogram on all classes.\n","        metrics (list[str] | str): Metrics to be evaluated, 'mIoU' and 'mDice'.\n","        nan_to_num (int, optional): If specified, NaN values will be replaced\n","            by the numbers defined by the user. Default: None.\n","     Returns:\n","        float: Overall accuracy on all images.\n","        ndarray: Per category accuracy, shape (num_classes, ).\n","        ndarray: Per category evaluation metrics, shape (num_classes, ).\n","    \"\"\"\n","    if isinstance(metrics, str):\n","        metrics = [metrics]\n","    allowed_metrics = ['mIoU', 'mDice', 'mFscore']\n","    if not set(metrics).issubset(set(allowed_metrics)):\n","        raise KeyError('metrics {} is not supported'.format(metrics))\n","\n","    all_acc = total_area_intersect.sum() / total_area_label.sum()\n","    ret_metrics = OrderedDict({'aAcc': all_acc})\n","    for metric in metrics:\n","        if metric == 'mIoU':\n","            iou = total_area_intersect / total_area_union\n","            acc = total_area_intersect / total_area_label\n","            ret_metrics['IoU'] = iou\n","            ret_metrics['Acc'] = acc\n","        elif metric == 'mDice':\n","            print(f'\\033[31\\nper_img_mdice:{per_img_mdice}\\033[39m')\n","            if per_img_mdice is None:\n","                dice = 2 * total_area_intersect / (\n","                    total_area_pred_label + total_area_label)\n","            else:\n","                dice = per_img_mdice\n","\n","            acc = total_area_intersect / total_area_label\n","            ret_metrics['Dice'] = dice\n","            ret_metrics['Acc'] = acc\n","        elif metric == 'mFscore':\n","            precision = total_area_intersect / total_area_pred_label\n","            recall = total_area_intersect / total_area_label\n","            f_value = torch.tensor(\n","                [f_score(x[0], x[1], beta) for x in zip(precision, recall)])\n","            ret_metrics['Fscore'] = f_value\n","            ret_metrics['Precision'] = precision\n","            ret_metrics['Recall'] = recall\n","\n","    ret_metrics = {\n","        metric: value.numpy()\n","        for metric, value in ret_metrics.items()\n","    }\n","    if nan_to_num is not None:\n","        ret_metrics = OrderedDict({\n","            metric: np.nan_to_num(metric_value, nan=nan_to_num)\n","            for metric, metric_value in ret_metrics.items()\n","        })\n","    return ret_metrics\n","\n","EOF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ccxIFSjYbWj"},"outputs":[],"source":["%%bash\n","cat << EOF > /content/mmsegmentation/mmseg/core/evaluation/metrics.py\n","# Copyright (c) OpenMMLab. All rights reserved.\n","from collections import OrderedDict\n","\n","import mmcv\n","import numpy as np\n","import torch\n","\n","\n","def f_score(precision, recall, beta=1):\n","    \"\"\"calculate the f-score value.\n","\n","    Args:\n","        precision (float | torch.Tensor): The precision value.\n","        recall (float | torch.Tensor): The recall value.\n","        beta (int): Determines the weight of recall in the combined score.\n","            Default: False.\n","\n","    Returns:\n","        [torch.tensor]: The f-score value.\n","    \"\"\"\n","    score = (1 + beta**2) * (precision * recall) / (\n","        (beta**2 * precision) + recall)\n","    return score\n","\n","\n","def intersect_and_union(pred_label,\n","                        label,\n","                        num_classes,\n","                        ignore_index,\n","                        label_map=dict(),\n","                        reduce_zero_label=False):\n","    \"\"\"Calculate intersection and Union.\n","\n","    Args:\n","        pred_label (ndarray | str): Prediction segmentation map\n","            or predict result filename.\n","        label (ndarray | str): Ground truth segmentation map\n","            or label filename.\n","        num_classes (int): Number of categories.\n","        ignore_index (int): Index that will be ignored in evaluation.\n","        label_map (dict): Mapping old labels to new labels. The parameter will\n","            work only when label is str. Default: dict().\n","        reduce_zero_label (bool): Whether ignore zero label. The parameter will\n","            work only when label is str. Default: False.\n","\n","     Returns:\n","         torch.Tensor: The intersection of prediction and ground truth\n","            histogram on all classes.\n","         torch.Tensor: The union of prediction and ground truth histogram on\n","            all classes.\n","         torch.Tensor: The prediction histogram on all classes.\n","         torch.Tensor: The ground truth histogram on all classes.\n","    \"\"\"\n","\n","    if isinstance(pred_label, str):\n","        pred_label = torch.from_numpy(np.load(pred_label))\n","    else:\n","        pred_label = torch.from_numpy((pred_label))\n","\n","    if isinstance(label, str):\n","        label = torch.from_numpy(\n","            mmcv.imread(label, flag='unchanged', backend='pillow'))\n","    else:\n","        label = torch.from_numpy(label)\n","\n","    if label_map is not None:\n","        for old_id, new_id in label_map.items():\n","            label[label == old_id] = new_id\n","    if reduce_zero_label:\n","        label[label == 0] = 255\n","        label = label - 1\n","        label[label == 254] = 255\n","\n","    mask = (label != ignore_index)\n","    pred_label = pred_label[mask]\n","    label = label[mask]\n","\n","    intersect = pred_label[pred_label == label]\n","    area_intersect = torch.histc(\n","        intersect.float(), bins=(num_classes), min=0, max=num_classes - 1)\n","    area_pred_label = torch.histc(\n","        pred_label.float(), bins=(num_classes), min=0, max=num_classes - 1)\n","    area_label = torch.histc(\n","        label.float(), bins=(num_classes), min=0, max=num_classes - 1)\n","    area_union = area_pred_label + area_label - area_intersect\n","    return area_intersect, area_union, area_pred_label, area_label\n","\n","\n","def total_intersect_and_union(results,\n","                              gt_seg_maps,\n","                              num_classes,\n","                              ignore_index,\n","                              label_map=dict(),\n","                              reduce_zero_label=False,\n","                              ):\n","    \"\"\"Calculate Total Intersection and Union.\n","\n","    Args:\n","        results (list[ndarray] | list[str]): List of prediction segmentation\n","            maps or list of prediction result filenames.\n","        gt_seg_maps (list[ndarray] | list[str] | Iterables): list of ground\n","            truth segmentation maps or list of label filenames.\n","        num_classes (int): Number of categories.\n","        ignore_index (int): Index that will be ignored in evaluation.\n","        label_map (dict): Mapping old labels to new labels. Default: dict().\n","        reduce_zero_label (bool): Whether ignore zero label. Default: False.\n","\n","     Returns:\n","         ndarray: The intersection of prediction and ground truth histogram\n","             on all classes.\n","         ndarray: The union of prediction and ground truth histogram on all\n","             classes.\n","         ndarray: The prediction histogram on all classes.\n","         ndarray: The ground truth histogram on all classes.\n","    \"\"\"\n","\n","    print('excuting <total_intersect_and_union>')\n","    total_area_intersect = torch.zeros((num_classes, ), dtype=torch.float64)\n","    total_area_union = torch.zeros((num_classes, ), dtype=torch.float64)\n","    total_area_pred_label = torch.zeros((num_classes, ), dtype=torch.float64)\n","    total_area_label = torch.zeros((num_classes, ), dtype=torch.float64)\n","    per_img_mdice = torch.zeros((num_classes, ), dtype=torch.float64)\n","\n","    cnt = 0\n","    for result, gt_seg_map in zip(results, gt_seg_maps):\n","        area_intersect, area_union, area_pred_label, area_label = \\\n","            intersect_and_union(\n","                result, gt_seg_map, num_classes, ignore_index,\n","                label_map, reduce_zero_label)\n","        total_area_intersect += area_intersect\n","        total_area_union += area_union\n","        total_area_pred_label += area_pred_label\n","        total_area_label += area_label\n","\n","        dice_coefficient = 2 * area_intersect / (area_pred_label + area_label)\n","        per_img_mdice += dice_coefficient\n","        cnt += 1\n","\n","    per_img_mdice /= cnt\n","    return total_area_intersect, total_area_union, total_area_pred_label, \\\n","        total_area_label, per_img_mdice\n","\n","\n","def mean_iou(results,\n","             gt_seg_maps,\n","             num_classes,\n","             ignore_index,\n","             nan_to_num=None,\n","             label_map=dict(),\n","             reduce_zero_label=False):\n","    \"\"\"Calculate Mean Intersection and Union (mIoU)\n","\n","    Args:\n","        results (list[ndarray] | list[str]): List of prediction segmentation\n","            maps or list of prediction result filenames.\n","        gt_seg_maps (list[ndarray] | list[str]): list of ground truth\n","            segmentation maps or list of label filenames.\n","        num_classes (int): Number of categories.\n","        ignore_index (int): Index that will be ignored in evaluation.\n","        nan_to_num (int, optional): If specified, NaN values will be replaced\n","            by the numbers defined by the user. Default: None.\n","        label_map (dict): Mapping old labels to new labels. Default: dict().\n","        reduce_zero_label (bool): Whether ignore zero label. Default: False.\n","\n","     Returns:\n","        dict[str, float | ndarray]:\n","            <aAcc> float: Overall accuracy on all images.\n","            <Acc> ndarray: Per category accuracy, shape (num_classes, ).\n","            <IoU> ndarray: Per category IoU, shape (num_classes, ).\n","    \"\"\"\n","    iou_result = eval_metrics(\n","        results=results,\n","        gt_seg_maps=gt_seg_maps,\n","        num_classes=num_classes,\n","        ignore_index=ignore_index,\n","        metrics=['mIoU'],\n","        nan_to_num=nan_to_num,\n","        label_map=label_map,\n","        reduce_zero_label=reduce_zero_label)\n","    return iou_result\n","\n","\n","def mean_dice(results,\n","              gt_seg_maps,\n","              num_classes,\n","              ignore_index,\n","              nan_to_num=None,\n","              label_map=dict(),\n","              reduce_zero_label=False):\n","    \"\"\"Calculate Mean Dice (mDice)\n","\n","    Args:\n","        results (list[ndarray] | list[str]): List of prediction segmentation\n","            maps or list of prediction result filenames.\n","        gt_seg_maps (list[ndarray] | list[str]): list of ground truth\n","            segmentation maps or list of label filenames.\n","        num_classes (int): Number of categories.\n","        ignore_index (int): Index that will be ignored in evaluation.\n","        nan_to_num (int, optional): If specified, NaN values will be replaced\n","            by the numbers defined by the user. Default: None.\n","        label_map (dict): Mapping old labels to new labels. Default: dict().\n","        reduce_zero_label (bool): Whether ignore zero label. Default: False.\n","\n","     Returns:\n","        dict[str, float | ndarray]: Default metrics.\n","            <aAcc> float: Overall accuracy on all images.\n","            <Acc> ndarray: Per category accuracy, shape (num_classes, ).\n","            <Dice> ndarray: Per category dice, shape (num_classes, ).\n","    \"\"\"\n","\n","    dice_result = eval_metrics(\n","        results=results,\n","        gt_seg_maps=gt_seg_maps,\n","        num_classes=num_classes,\n","        ignore_index=ignore_index,\n","        metrics=['mDice'],\n","        nan_to_num=nan_to_num,\n","        label_map=label_map,\n","        reduce_zero_label=reduce_zero_label)\n","    return dice_result\n","\n","\n","def mean_fscore(results,\n","                gt_seg_maps,\n","                num_classes,\n","                ignore_index,\n","                nan_to_num=None,\n","                label_map=dict(),\n","                reduce_zero_label=False,\n","                beta=1):\n","    \"\"\"Calculate Mean F-Score (mFscore)\n","\n","    Args:\n","        results (list[ndarray] | list[str]): List of prediction segmentation\n","            maps or list of prediction result filenames.\n","        gt_seg_maps (list[ndarray] | list[str]): list of ground truth\n","            segmentation maps or list of label filenames.\n","        num_classes (int): Number of categories.\n","        ignore_index (int): Index that will be ignored in evaluation.\n","        nan_to_num (int, optional): If specified, NaN values will be replaced\n","            by the numbers defined by the user. Default: None.\n","        label_map (dict): Mapping old labels to new labels. Default: dict().\n","        reduce_zero_label (bool): Whether ignore zero label. Default: False.\n","        beta (int): Determines the weight of recall in the combined score.\n","            Default: False.\n","\n","\n","     Returns:\n","        dict[str, float | ndarray]: Default metrics.\n","            <aAcc> float: Overall accuracy on all images.\n","            <Fscore> ndarray: Per category recall, shape (num_classes, ).\n","            <Precision> ndarray: Per category precision, shape (num_classes, ).\n","            <Recall> ndarray: Per category f-score, shape (num_classes, ).\n","    \"\"\"\n","    fscore_result = eval_metrics(\n","        results=results,\n","        gt_seg_maps=gt_seg_maps,\n","        num_classes=num_classes,\n","        ignore_index=ignore_index,\n","        metrics=['mFscore'],\n","        nan_to_num=nan_to_num,\n","        label_map=label_map,\n","        reduce_zero_label=reduce_zero_label,\n","        beta=beta)\n","    return fscore_result\n","\n","\n","def eval_metrics(results,\n","                 gt_seg_maps,\n","                 num_classes,\n","                 ignore_index,\n","                 metrics=['mIoU'],\n","                 nan_to_num=None,\n","                 label_map=dict(),\n","                 reduce_zero_label=False,\n","                 beta=1):\n","    \"\"\"Calculate evaluation metrics\n","    Args:\n","        results (list[ndarray] | list[str]): List of prediction segmentation\n","            maps or list of prediction result filenames.\n","        gt_seg_maps (list[ndarray] | list[str] | Iterables): list of ground\n","            truth segmentation maps or list of label filenames.\n","        num_classes (int): Number of categories.\n","        ignore_index (int): Index that will be ignored in evaluation.\n","        metrics (list[str] | str): Metrics to be evaluated, 'mIoU' and 'mDice'.\n","        nan_to_num (int, optional): If specified, NaN values will be replaced\n","            by the numbers defined by the user. Default: None.\n","        label_map (dict): Mapping old labels to new labels. Default: dict().\n","        reduce_zero_label (bool): Whether ignore zero label. Default: False.\n","     Returns:\n","        float: Overall accuracy on all images.\n","        ndarray: Per category accuracy, shape (num_classes, ).\n","        ndarray: Per category evaluation metrics, shape (num_classes, ).\n","    \"\"\"\n","\n","    total_area_intersect, total_area_union, total_area_pred_label, \\\n","        total_area_label, per_img_mdice = total_intersect_and_union(\n","            results, gt_seg_maps, num_classes, ignore_index, label_map,\n","            reduce_zero_label)\n","    ret_metrics = total_area_to_metrics(total_area_intersect, total_area_union,\n","                                        total_area_pred_label,\n","                                        total_area_label,\n","                                        per_img_mdice,\n","                                        metrics, nan_to_num,\n","                                        beta)\n","\n","    return ret_metrics\n","\n","\n","def pre_eval_to_metrics(pre_eval_results,\n","                        metrics=['mIoU'],\n","                        nan_to_num=None,\n","                        beta=1):\n","    \"\"\"Convert pre-eval results to metrics.\n","\n","    Args:\n","        pre_eval_results (list[tuple[torch.Tensor]]): per image eval results\n","            for computing evaluation metric\n","        metrics (list[str] | str): Metrics to be evaluated, 'mIoU' and 'mDice'.\n","        nan_to_num (int, optional): If specified, NaN values will be replaced\n","            by the numbers defined by the user. Default: None.\n","     Returns:\n","        float: Overall accuracy on all images.\n","        ndarray: Per category accuracy, shape (num_classes, ).\n","        ndarray: Per category evaluation metrics, shape (num_classes, ).\n","    \"\"\"\n","\n","    # convert list of tuples to tuple of lists, e.g.\n","    # [(A_1, B_1, C_1, D_1), ...,  (A_n, B_n, C_n, D_n)] to\n","    # ([A_1, ..., A_n], ..., [D_1, ..., D_n])\n","    print('\\nexcuting <pre_eval_to_metrics>')\n","    pre_eval_results = tuple(zip(*pre_eval_results))\n","    assert len(pre_eval_results) == 4\n","\n","    total_area_intersect = sum(pre_eval_results[0])\n","    total_area_union = sum(pre_eval_results[1])\n","    total_area_pred_label = sum(pre_eval_results[2])\n","    total_area_label = sum(pre_eval_results[3])\n","    per_img_mdice = None  # sum(pre_eval_results[4])\n","\n","    ret_metrics = total_area_to_metrics(total_area_intersect, total_area_union,\n","                                        total_area_pred_label,\n","                                        total_area_label,\n","                                        per_img_mdice,\n","                                        metrics, nan_to_num,\n","                                        beta)\n","\n","    return ret_metrics\n","\n","\n","def total_area_to_metrics(total_area_intersect,\n","                          total_area_union,\n","                          total_area_pred_label,\n","                          total_area_label,\n","                          per_img_mdice,\n","                          metrics=['mIoU'],\n","                          nan_to_num=None,\n","                          beta=1):\n","    \"\"\"Calculate evaluation metrics\n","    Args:\n","        total_area_intersect (ndarray): The intersection of prediction and\n","            ground truth histogram on all classes.\n","        total_area_union (ndarray): The union of prediction and ground truth\n","            histogram on all classes.\n","        total_area_pred_label (ndarray): The prediction histogram on all\n","            classes.\n","        total_area_label (ndarray): The ground truth histogram on all classes.\n","        metrics (list[str] | str): Metrics to be evaluated, 'mIoU' and 'mDice'.\n","        nan_to_num (int, optional): If specified, NaN values will be replaced\n","            by the numbers defined by the user. Default: None.\n","     Returns:\n","        float: Overall accuracy on all images.\n","        ndarray: Per category accuracy, shape (num_classes, ).\n","        ndarray: Per category evaluation metrics, shape (num_classes, ).\n","    \"\"\"\n","    if isinstance(metrics, str):\n","        metrics = [metrics]\n","    allowed_metrics = ['mIoU', 'mDice', 'mFscore']\n","    if not set(metrics).issubset(set(allowed_metrics)):\n","        raise KeyError('metrics {} is not supported'.format(metrics))\n","\n","    all_acc = total_area_intersect.sum() / total_area_label.sum()\n","    ret_metrics = OrderedDict({'aAcc': all_acc})\n","    for metric in metrics:\n","        if metric == 'mIoU':\n","            iou = total_area_intersect / total_area_union\n","            acc = total_area_intersect / total_area_label\n","            ret_metrics['IoU'] = iou\n","            ret_metrics['Acc'] = acc\n","        elif metric == 'mDice':\n","            print(f'\\033[31\\nper_img_mdice:{per_img_mdice}\\033[39m')\n","            if per_img_mdice is None:\n","                dice = 2 * total_area_intersect / (\n","                    total_area_pred_label + total_area_label)\n","            else:\n","                dice = per_img_mdice\n","\n","            acc = total_area_intersect / total_area_label\n","            ret_metrics['Dice'] = dice\n","            ret_metrics['Acc'] = acc\n","        elif metric == 'mFscore':\n","            precision = total_area_intersect / total_area_pred_label\n","            recall = total_area_intersect / total_area_label\n","            f_value = torch.tensor(\n","                [f_score(x[0], x[1], beta) for x in zip(precision, recall)])\n","            ret_metrics['Fscore'] = f_value\n","            ret_metrics['Precision'] = precision\n","            ret_metrics['Recall'] = recall\n","\n","    ret_metrics = {\n","        metric: value.numpy()\n","        for metric, value in ret_metrics.items()\n","    }\n","    if nan_to_num is not None:\n","        ret_metrics = OrderedDict({\n","            metric: np.nan_to_num(metric_value, nan=nan_to_num)\n","            for metric, metric_value in ret_metrics.items()\n","        })\n","    return ret_metrics\n","\n","EOF"]},{"cell_type":"markdown","metadata":{"id":"x-EnYSaKKVIU"},"source":["## Prepare configs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5uDxvI6dmP3"},"outputs":[],"source":["%%bash\n","\n","cat << EOF > /content/mmsegmentation/configs/deeplabv3plus_r50-d8_512x512_hubmap.py\n","\n","#-------------------------------------------------------------------------\n","# ../_base_/models/deeplabv3plus_r50-d8.py\n","# model settings\n","norm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\n","model = dict(\n","    type='EncoderDecoder',\n","    backbone=dict(\n","        type='MixVisionTransformer',\n","        # init_cfg=dict(\n","        #     type='Pretrained',\n","        #     checkpoint='https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segformer/mit_b5_20220624-658746d9.pth'\n","        #     ),\n","        in_channels=3,\n","        embed_dims=64,\n","        num_stages=4,\n","        num_layers=[3, 6, 40, 3],\n","        num_heads=[1, 2, 5, 8],\n","        patch_sizes=[7, 3, 3, 3],\n","        sr_ratios=[8, 4, 2, 1],\n","        out_indices=(0, 1, 2, 3),\n","        mlp_ratio=4,\n","        qkv_bias=True,\n","        drop_rate=0.0,\n","        attn_drop_rate=0.0,\n","        drop_path_rate=0.1),\n","    decode_head=dict(\n","        type='SegformerHead',\n","        in_channels=[64, 128, 320, 512],\n","        in_index=[0, 1, 2, 3],\n","        channels=256,\n","        dropout_ratio=0.1,\n","        num_classes=2,\n","        norm_cfg=norm_cfg,\n","        align_corners=False,\n","        loss_decode=[\n","            dict(type='CrossEntropyLoss', loss_name='loss_ce', loss_weight=1),\n","            dict(type='LovaszLoss', loss_name='loss_lovasz', per_image=True, loss_weight=3),\n","        ]\n","        ),\n","    # model training and testing settings\n","    train_cfg=dict(),\n","    test_cfg=dict(mode='whole'))\n","\n","\n","\n","#-------------------------------------------------------------------------\n","# _base_/datasets/pascal_voc12_aug.py\n","# dataset settings\n","\n","\n","dataset_type = 'HubmapDataset'\n","data_root = '/content'\n","img_norm_cfg = dict(\n","    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n","crop_size = (960, 960)\n","train_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(type='LoadAnnotations'),\n","    dict(type='OrgansDataAug'),\n","    dict(type='Resize', img_scale=[(480,480),(1600, 1600)], keep_ratio=False, ratio_range=None),\n","    dict(type='RandomCrop', crop_size=crop_size, cat_max_ratio=0.5), #cat_max_ratio=0.75\n","    dict(type='RandomFlip', direction='horizontal', prob=0),\n","    #dict(type='RandomRotate', degree=180, prob=0.7),\n","    dict(type='SaveOverlay', save_root_dir='/content', save_num=500, no_overlay=True),\n","    dict(type='Convert2Class1'),\n","    dict(type='Normalize', **img_norm_cfg),\n","    dict(type='Pad', size=crop_size, pad_val=0, seg_pad_val=255),\n","    dict(type='DefaultFormatBundle'),\n","    dict(type='Collect', keys=['img', 'gt_semantic_seg']),\n","]\n","test_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(\n","        type='MultiScaleFlipAug',\n","        img_scale=crop_size, #画像のリサイズ(モデルのinputサイズではない)\n","        # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n","        flip=True,\n","        transforms=[\n","            dict(type='Resize', keep_ratio=True),\n","            dict(type='RandomFlip'),\n","            dict(type='Normalize', **img_norm_cfg),\n","            dict(type='ImageToTensor', keys=['img']),\n","            dict(type='Collect', keys=['img']),\n","        ])\n","]\n","\n","\n","data = dict(\n","    samples_per_gpu=1, #batch\n","    workers_per_gpu=2,\n","    train=dict(\n","        type=dataset_type,\n","        data_root=data_root,\n","        img_dir=[\n","            'hubmap_multi_2000x2000/images',\n","            'hubmap_multi_2000x2000/images_stained_with_hubmap1',\n","            'hubmap_multi_2000x2000/images_stained_with_hubmap2',\n","            'hubmap_multi_2000x2000/images_stained_with_sample1',\n","            'hubmap_multi_2000x2000/images_stained_with_test',\n","            'external_spleen_v2/image',\n","            'external_lung_v1/image'\n","        ],\n","        ann_dir=[\n","            'hubmap_multi_2000x2000/lung_refined_masks',\n","            'hubmap_multi_2000x2000/lung_refined_masks',\n","            'hubmap_multi_2000x2000/lung_refined_masks',\n","            'hubmap_multi_2000x2000/lung_refined_masks',\n","            'hubmap_multi_2000x2000/lung_refined_masks',\n","            'external_spleen_v2/mask',\n","            'external_lung_v1/mask_0.3'\n","        ],\n","        img_suffix='.png',\n","        seg_map_suffix='.png',\n","        split= [\n","            '/content/hubmap_multi_2000x2000/ImageSets/Segmentation/trainval.txt',\n","            '/content/hubmap_multi_2000x2000/ImageSets/Segmentation/trainval.txt',\n","            '/content/hubmap_multi_2000x2000/ImageSets/Segmentation/trainval.txt',\n","            '/content/hubmap_multi_2000x2000/ImageSets/Segmentation/trainval.txt',\n","            '/content/hubmap_multi_2000x2000/ImageSets/Segmentation/trainval.txt',\n","            '/content/external_spleen_v2/trainval.txt',\n","            '/content/external_lung_v1/external_lung.txt'\n","        ],\n","        pipeline=train_pipeline),\n","    val=dict(\n","        type=dataset_type,\n","        data_root=data_root,\n","        img_dir='hubmap_multi_2000x2000/images_stained_with_hubmap1',\n","        ann_dir='hubmap_multi_2000x2000/lung_refined_masks_2class',\n","        img_suffix='.png',\n","        seg_map_suffix='.png',\n","        split='/content/hubmap_multi_2000x2000/ImageSets/Segmentation/val_fold0.txt',\n","        pipeline=test_pipeline),\n","    test=dict()\n","    )\n","\n","#-------------------------------------------------------------------------\n","# '../_base_/schedules/schedule_20k.py'\n","\n","# optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0005)\n","optimizer_config = dict()\n","# lr_config = dict(warmup='linear', warmup_iters=500, by_epoch=False, \n","#                  policy='poly', power=0.9, min_lr=0.001)\n","\n","optimizer = dict(\n","    type='AdamW',\n","    lr=0.00006,#0.00006\n","    betas=(0.9, 0.999),\n","    weight_decay=0.01,\n","    paramwise_cfg=dict(\n","        custom_keys={\n","            'pos_block': dict(decay_mult=0.),\n","            'norm': dict(decay_mult=0.),\n","            'head': dict(lr_mult=10.)\n","        }))\n","\n","lr_config = dict(\n","    policy='poly',\n","    warmup='linear',\n","    warmup_iters=1500,\n","    warmup_ratio=1e-6,\n","    power=1.0,\n","    min_lr=0,\n","    by_epoch=False)\n","\n","\n","# runtime settings\n","runner = dict(type='IterBasedRunner', max_iters=45000)\n","checkpoint_config = dict(by_epoch=False, interval=45000) #max_keep_ckpts\n","evaluation = dict(interval=3000, metric=['mDice', 'mIoU'], pre_eval=False) #save_best='mIoU'\n","\n","#-------------------------------------------------------------------------\n","# _base_/default_runtime.py\n","\n","log_config = dict(\n","    interval=10,\n","    hooks=[\n","        dict(type='TextLoggerHook', by_epoch=False),\n","\n","        dict(type='MMSegWandbHook',\n","                init_kwargs=dict(project=\"HubMap-960\",\n","                                 name=f'mit-b5-960',\n","                                 config={'config':'deeplabv3plus_r50-d8_512x512_hubmap.py',\n","                                         'comment':'No comment',\n","                                         'dataset_type': dataset_type,\n","                                         'model': model,\n","                                         'crop_size': crop_size,\n","                                         'train_pipeline': train_pipeline,\n","                                         'test_pipeline': test_pipeline,\n","                                         'optimizer':optimizer,\n","                                         'lr_config':lr_config,\n","                                         'runner': runner,\n","                                         'checkpoint_config': checkpoint_config,\n","                                         'evaluation':evaluation,\n","                                         'data': data,\n","                                         },\n","                                 #group='',\n","                                 entity=None),\n","                interval=3000,\n","                log_checkpoint=True,\n","                log_checkpoint_metadata=True,\n","                num_eval_images=100)\n","    ]\n","    )\n","\n","dist_params = dict(backend='nccl')\n","log_level = 'INFO'\n","load_from = '/content/segformer_mit-b5_8x1_1024x1024_160k_cityscapes_20211206_072934-87a052ec.pth'\n","resume_from = None\n","workflow = [('train', 1)]\n","cudnn_benchmark = True\n","\n","\n","\n","#-------------------------------------------------------------------------\n","work_dir='/content/hubmap_training'\n","\n","from mmseg.apis import set_random_seed\n","set_random_seed(0, deterministic=False)\n","\n","# 必須\n","seed = 0\n","gpu_ids = range(1)\n","device='cuda'\n","\n","EOF"]},{"cell_type":"markdown","metadata":{"id":"QWuH14LYF2gQ"},"source":["## Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Cl3fzvmidhdp"},"outputs":[],"source":["%cd /content/mmsegmentation\n","\n","config='configs/deeplabv3plus_r50-d8_512x512_hubmap.py'\n","\n","!rm -r /content/overlay\n","!python tools/train.py {config}"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":["D8ZWOUR2Yb_2"],"machine_shape":"hm","provenance":[{"file_id":"https://github.com/open-mmlab/mmsegmentation/blob/master/demo/MMSegmentation_Tutorial.ipynb","timestamp":1652616460602}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}}},"nbformat":4,"nbformat_minor":0}