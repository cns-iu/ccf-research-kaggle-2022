{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.metrics import jaccard_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# <h6> Step 2 - Write utility functions </h6> \n",
    "def enc2mask(encs, shape):\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for m, enc in enumerate(encs):\n",
    "        if isinstance(enc, float) and np.isnan(enc):\n",
    "            continue\n",
    "        enc_split = enc.split()\n",
    "        for i in range(len(enc_split) // 2):\n",
    "            start = int(enc_split[2 * i]) - 1\n",
    "            length = int(enc_split[2 * i + 1])\n",
    "            img[start: start + length] = 1 + m\n",
    "    return img.reshape(shape).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_scores_img(pred, truth, eps=1e-8):\n",
    "    pred = pred.reshape(-1) > 0\n",
    "    truth = truth.reshape(-1) > 0\n",
    "    intersect = (pred & truth).sum(-1)\n",
    "    union = pred.sum(-1) + truth.sum(-1)\n",
    "    dice = (2.0 * intersect + eps) / (union + eps)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perf_metrics(gt, pred):\n",
    "    n = 0\n",
    "    d = 0\n",
    "    for i in range(gt.shape[0]):\n",
    "        for j in range (gt.shape[1]):\n",
    "            if (gt[i][j]==pred[i][j]):\n",
    "                n = n+1\n",
    "            d = d+1\n",
    "    return n/d, jaccard_score(gt.flatten(order='C'), pred.flatten(order='C'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(mask_truth, mask_pred):\n",
    "    mask1 = mask_truth.flatten()\n",
    "    mask2 = mask_pred.flatten()\n",
    "\n",
    "    # calculate the confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(mask1, mask2, labels=[0, 1]).ravel()\n",
    "    return tn, fp, fn, tp\n",
    "\n",
    "def get_hausdorff_distance(mask_truth, mask_pred):\n",
    "    # find coordinates of non-zero elements in the masks\n",
    "    gt_coords = np.argwhere(mask_truth)\n",
    "    pred_coords = np.argwhere(mask_pred)\n",
    "\n",
    "    # calculate directed Hausdorff distance from gt to pred\n",
    "    hausdorff_gt_to_pred = directed_hausdorff(gt_coords, pred_coords)[0]\n",
    "    #hausdorff_gt_to_pred_mask = directed_hausdorff(mask_truth, mask_pred)[0] # incorrect\n",
    "\n",
    "    # calculate directed Hausdorff distance from pred to gt\n",
    "    hausdorff_pred_to_gt = directed_hausdorff(pred_coords, gt_coords)[0]\n",
    "\n",
    "    # take the 95th percentile Hausdorff distance\n",
    "    hausdorff_distance_95 = np.percentile([hausdorff_pred_to_gt, hausdorff_gt_to_pred], 95)\n",
    "\n",
    "    return hausdorff_gt_to_pred, hausdorff_pred_to_gt, hausdorff_distance_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Step 3 - Calculate mean metrics values for test images \n",
    "BASE_PATH = os.getcwd()+'/'\n",
    "INPUT_PATH = BASE_PATH\n",
    "print(INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_metadata = pd.read_csv(INPUT_PATH+'dataset_split_metadata/all_metadata_for_publication.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = df_metadata[df_metadata[\"Usage\"] == \"private_test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = df_metadata[[\"filename\", \"tissue_name\", \"image_dims\", \"rle\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = df_info.rename(columns={'filename': 'id'})\n",
    "df_info['filename'] = df_info['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_metadata_dir = \"dataset_split_metadata/\"\n",
    "with open(json_metadata_dir + \"old_to_new_id_map.json\") as f:\n",
    "    mapping_json = json.load(f) \n",
    "mapping_json_inv = {int(v): k for k, v in mapping_json.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info['id'] = df_info['id'].map(mapping_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_truth = df_info[[\"id\", \"rle\", \"filename\", \"tissue_name\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info.reset_index(drop=True, inplace=True)\n",
    "df_truth.reset_index(drop=True, inplace=True)\n",
    "df_truth = df_truth.astype({\"id\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6,51):\n",
    "    team = f\"team_{i}\"\n",
    "    df_pred = pd.read_csv(INPUT_PATH+f'winning-submissions/other_top50_teams/{team}/submission.csv')\n",
    "    \n",
    "    df_pred.reset_index(drop=True, inplace=True)\n",
    "    df_pred = df_pred.astype({\"id\": int})\n",
    "    \n",
    "    scores = {}\n",
    "    iou_list = {}\n",
    "    hd_pred_to_gt_list = {}\n",
    "    hd_95_list = {}\n",
    "    count = 0\n",
    "    for j in df_info.index:\n",
    "        \n",
    "        id = df_info.at[j, \"id\"]\n",
    "        shape = df_info.at[j, \"image_dims\"]\n",
    "        shape = shape.strip().strip('(').strip(')').split(',')\n",
    "        shape = [int(shape[0]), int(shape[1])]\n",
    "        id_int = int(id)\n",
    "        truth = df_truth[df_truth['id'] == id_int]['rle']\n",
    "\n",
    "        mask_truth = enc2mask(truth, shape)\n",
    "        \n",
    "        pred = df_pred[df_pred['id'] == id_int]['rle']\n",
    "        mask_pred = enc2mask(pred, shape) \n",
    "        \n",
    "        score = dice_scores_img(mask_pred, mask_truth)\n",
    "        _, iou_score = perf_metrics(mask_truth, mask_pred)\n",
    "        \n",
    "        hd_gt_to_pred, hd_pred_to_gt, hd_95 = get_hausdorff_distance(mask_truth, mask_pred)\n",
    "        \n",
    "        scores[id] = score\n",
    "        iou_list[id] = iou_score\n",
    "        hd_pred_to_gt_list[id] = hd_pred_to_gt\n",
    "        hd_95_list[id] = hd_95\n",
    "        count +=1\n",
    "        # print(id, count)\n",
    "  \n",
    "\n",
    "    # Export dataframes to CSVs.\n",
    "\n",
    "    #1. Export pred and gt mask with id\n",
    "    \n",
    "    df_masks = df_truth.merge(df_pred, on='id', how='left', suffixes=('_truth', '_pred'))\n",
    "    df_masks.to_csv(INPUT_PATH+f'winning-submissions/other_top50_teams/{team}/masks.csv',index=None)\n",
    "\n",
    "    #2. Export dice and IOU value with id, filename, organ.\n",
    "    # create a list of tuples from the dictionary\n",
    "    scores_list_tuples = [(key, value) for key, value in scores.items()]\n",
    "    iou_list_tuples = [(key, value) for key, value in iou_list.items()]\n",
    "    hd_list_tuples = [(key, value) for key, value in hd_pred_to_gt_list.items()]\n",
    "    hd95_list_tuples = [(key, value) for key, value in hd_95_list.items()]\n",
    "    # create a pandas dataframe from the list\n",
    "    df_scores_list_tuples = pd.DataFrame(scores_list_tuples, columns=['id', 'dice_score'])\n",
    "    df_iou_list_tuples = pd.DataFrame(iou_list_tuples, columns=['id', 'iou_score'])\n",
    "    df_hd_list_tuples = pd.DataFrame(hd_list_tuples, columns=['id', 'hd_score'])\n",
    "    df_hd95_list_tuples = pd.DataFrame(hd95_list_tuples, columns=['id', 'hd95_score'])\n",
    "\n",
    "    df_scores_list_tuples = df_scores_list_tuples.astype({\"id\": int})\n",
    "    df_iou_list_tuples = df_iou_list_tuples.astype({\"id\": int})\n",
    "    df_hd_list_tuples = df_hd_list_tuples.astype({\"id\": int})\n",
    "    df_hd95_list_tuples = df_hd95_list_tuples.astype({\"id\": int})\n",
    "\n",
    "    df_metrics_temp = df_scores_list_tuples.merge(df_iou_list_tuples, on='id', how='left')\n",
    "    df_metrics_temp = df_metrics_temp.merge(df_hd_list_tuples, on='id', how='left')\n",
    "    df_metrics_temp = df_metrics_temp.merge(df_hd95_list_tuples, on='id', how='left')\n",
    "    df_metrics = df_metrics_temp.merge(df_truth, on='id', how='left', suffixes=('_truth', '_pred'))\n",
    "\n",
    "    # drop the 'rle' column\n",
    "    df_metrics = df_metrics.drop('rle', axis=1)\n",
    "    df_metrics.to_csv(INPUT_PATH+f'winning-submissions/other_top50_teams/{team}/metrics.csv',index=None)\n",
    "\n",
    "\n",
    "    print(f\"Team completed: {team}\")\n",
    "    \n",
    "print(\"Metric computation complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tom-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b36821e70226ba44ed300b7ad44e27b6d77f6807368f9c95217b0b84df56ccd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
